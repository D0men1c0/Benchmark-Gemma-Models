{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b64f86a",
   "metadata": {},
   "source": [
    "## Benchmark Results Visualization\n",
    "\n",
    "This notebook loads and visualizes the results from the LLM benchmark runs.\n",
    "It can handle results stored in JSON or CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eda5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (6, 8) # Default figure size\n",
    "\n",
    "RESULTS_DIR = \"../benchmarks_output\"\n",
    "RESULTS_BASE_NAME = \"benchmark_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00caf28a",
   "metadata": {},
   "source": [
    "## 1. Configuration and Result File Loading\n",
    " \n",
    "Specify the **directory** where your benchmark results are saved and the **base name** of the results file.\n",
    "The notebook will attempt to load `.json` or `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5ad53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_data(results_dir: str, base_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load benchmark results from JSON or CSV files.\n",
    "    :param results_dir: Directory containing the results files.\n",
    "    :param base_name: Base name of the results files (without extension).\n",
    "    :return: DataFrame containing the benchmark results, or None if no files found.\n",
    "    \"\"\"\n",
    "    path_json = Path(results_dir) / f\"{base_name}.json\"\n",
    "    path_csv = Path(results_dir) / f\"{base_name}.csv\"\n",
    "\n",
    "    try:\n",
    "        if path_json.exists():\n",
    "            print(f\"Loading JSON: {path_json}\")\n",
    "            with open(path_json, 'r') as f:\n",
    "                return flatten_json_results(json.load(f))\n",
    "        elif path_csv.exists():\n",
    "            print(f\"Loading CSV: {path_csv}\")\n",
    "            df = pd.read_csv(path_csv)\n",
    "            return reshape_csv_results(df) if 'score' not in df.columns else df.assign(score=pd.to_numeric(df['score'], errors='coerce'))\n",
    "        else:\n",
    "            print(f\"No result files found in {results_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Load error: {e}\")\n",
    "    return None\n",
    "\n",
    "def flatten_json_results(data: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten the nested JSON structure into a DataFrame.\n",
    "    :param data: Nested dictionary containing benchmark results.\n",
    "    :return: DataFrame with columns for model, task, metric, sub-metric, and score.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for model, tasks in data.items():\n",
    "        if not isinstance(tasks, dict): continue\n",
    "        for task, metrics in tasks.items():\n",
    "            if not isinstance(metrics, dict): continue\n",
    "            base = {\"model\": model, \"task\": task}\n",
    "            if \"error\" in metrics:\n",
    "                print(f\"Error in {model}/{task}: {metrics['error']}\")\n",
    "            for metric, val in metrics.items():\n",
    "                if metric == \"error\": continue\n",
    "                if isinstance(val, dict):\n",
    "                    for sub, score in val.items():\n",
    "                        records.append({**base, \"metric\": metric, \"sub_metric\": sub, \"score\": to_float(score)})\n",
    "                else:\n",
    "                    records.append({**base, \"metric\": metric, \"sub_metric\": None, \"score\": to_float(val)})\n",
    "    df = pd.DataFrame(records)\n",
    "    if not df.empty:\n",
    "        df[\"full_metric_name\"] = df.apply(lambda r: f\"{r['metric']}_{r['sub_metric']}\" if r['sub_metric'] else r['metric'], axis=1)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"model\", \"task\", \"metric\", \"sub_metric\", \"score\", \"full_metric_name\"])\n",
    "    return df\n",
    "\n",
    "def reshape_csv_results(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reshape the DataFrame from wide to long format.\n",
    "    :param df: DataFrame containing benchmark results.\n",
    "    :return: Reshaped DataFrame with columns for model, task, metric, sub-metric, and score.\n",
    "    \"\"\"\n",
    "    id_vars = [col for col in ['model', 'task'] if col in df.columns]\n",
    "    if not id_vars:\n",
    "        if all(col in df.columns for col in ['score', 'metric']):\n",
    "            df['full_metric_name'] = df.apply(lambda r: f\"{r['metric']}_{r['sub_metric']}\" if r.get('sub_metric') else r['metric'], axis=1)\n",
    "        return df\n",
    "\n",
    "    df_long = pd.melt(df, id_vars=id_vars, var_name='full_metric_name', value_name='score')\n",
    "    df_long[['metric', 'sub_metric']] = df_long['full_metric_name'].apply(\n",
    "        lambda name: pd.Series(split_metric_name(name))\n",
    "    )\n",
    "    df_long['score'] = pd.to_numeric(df_long['score'], errors='coerce')\n",
    "    return df_long\n",
    "\n",
    "def split_metric_name(name: str) -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Split the metric name into base metric and sub-metric.\n",
    "    :param name: Full metric name (e.g., \"rouge1\", \"bert_score_f1\").\n",
    "    :return: Tuple of (base metric, sub-metric) or (name, None) if no sub-metric.\n",
    "    \"\"\"\n",
    "    parts = name.split('_', 1)\n",
    "    return (parts[0], parts[1]) if len(parts) == 2 and parts[0] in ['rouge', 'bert_score'] else (name, None)\n",
    "\n",
    "def to_float(value: Any) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert a value to float, returning None if conversion fails.\n",
    "    :param value: Value to convert.\n",
    "    :return: Converted float value or None.\n",
    "    \"\"\"\n",
    "    try: return float(value)\n",
    "    except (ValueError, TypeError): return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fca711",
   "metadata": {},
   "source": [
    "## 2. Load Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c24f7b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON: ..\\benchmarks_output\\benchmark_results.json\n",
      "\n",
      "Processed DataFrame Head:\n",
      "               model                           task       metric sub_metric  \\\n",
      "0  gemma-7b - 4quant  MMLU (all Subset - Templated)  exact_match       None   \n",
      "1  gemma-7b - 4quant  MMLU (all Subset - Templated)     accuracy       None   \n",
      "2  gemma-7b - 4quant  MMLU (all Subset - Templated)     f1_score       None   \n",
      "3  gemma-7b - 4quant  MMLU (all Subset - Templated)    precision       None   \n",
      "4  gemma-7b - 4quant  MMLU (all Subset - Templated)       recall       None   \n",
      "\n",
      "      score full_metric_name  \n",
      "0  0.300000      exact_match  \n",
      "1  0.300000         accuracy  \n",
      "2  0.185025         f1_score  \n",
      "3  0.212609        precision  \n",
      "4  0.300000           recall  \n",
      "\n",
      "DataFrame shape: (675, 6)\n",
      "\n",
      "Available Models: ['gemma-7b - 4quant' 'gemma-2b - 4quant' 'gpt2 full prec' 'phi-2 - 4quant'\n",
      " 'mistral-7b - 4quant' 'mistral-7b - 8quant' 'llama3-8b - 8quant'\n",
      " 'llama2-7b - 4quant' 'llama3-8b - 4quant' 'falcon - 4quant'\n",
      " 'falcon - 8quant']\n",
      "Available Tasks: ['MMLU (all Subset - Templated)' 'GSM8K (Reasoning Text Analysis)'\n",
      " 'CNN/DailyMail Summarization' 'OPUS-100 English-to-French'\n",
      " 'GLUE SST-2 (Prompting - Improved)' 'GLUE MRPC (Prompting - Improved)'\n",
      " 'GLUE STS-B (Prompting - Improved)']\n",
      "Available Metrics (full_metric_name): ['exact_match' 'accuracy' 'f1_score' 'precision' 'recall' 'rouge_rouge1_f'\n",
      " 'rouge_rouge1_p' 'rouge_rouge1_r' 'rouge_rouge2_f' 'rouge_rouge2_p'\n",
      " 'rouge_rouge2_r' 'rouge_rougeL_f' 'rouge_rougeL_p' 'rouge_rougeL_r'\n",
      " 'bert_score_bertscore_precision' 'bert_score_bertscore_recall'\n",
      " 'bert_score_bertscore_f1' 'distinct_ngram_distinct_1'\n",
      " 'distinct_ngram_distinct_2' 'distinct_ngram_distinct_3' 'word_entropy'\n",
      " 'toxicity_toxic' 'rouge_rougeLsum_f' 'rouge_rougeLsum_p'\n",
      " 'rouge_rougeLsum_r' 'meteor' 'bleu' 'distinct_ngram_distinct_4'\n",
      " 'semantic_similarity' 'pearson_correlation' 'spearman_correlation']\n"
     ]
    }
   ],
   "source": [
    "results_df = load_benchmark_data(RESULTS_DIR, RESULTS_BASE_NAME)\n",
    "\n",
    "if results_df is not None and not results_df.empty:\n",
    "    print(\"\\nProcessed DataFrame Head:\")\n",
    "    print(results_df.head())\n",
    "    print(f\"\\nDataFrame shape: {results_df.shape}\")\n",
    "    print(\"\\nAvailable Models:\", results_df['model'].unique())\n",
    "    print(\"Available Tasks:\", results_df['task'].unique())\n",
    "    # Check if 'full_metric_name' column exists before trying to access its unique values\n",
    "    if 'full_metric_name' in results_df.columns:\n",
    "        print(\"Available Metrics (full_metric_name):\", results_df['full_metric_name'].unique())\n",
    "    else:\n",
    "        print(\"Warning: 'full_metric_name' column not found in DataFrame. Plotting might be affected.\")\n",
    "else:\n",
    "    print(\"No data loaded or DataFrame is empty. Cannot proceed with plotting.\")\n",
    "    # Create an empty DataFrame with expected columns to prevent errors in plotting functions if they are called\n",
    "    results_df = pd.DataFrame(columns=[\"model\", \"task\", \"metric\", \"sub_metric\", \"score\", \"full_metric_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0323d4",
   "metadata": {},
   "source": [
    "## 3. Visualizations\n",
    " \n",
    "Create various plots to compare model performance.\n",
    "The plotting functions below should work with the DataFrame structure produced by `load_benchmark_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1a3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_metrics(df: pd.DataFrame, task_name: str, metrics: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Plot metrics for a specific task.\n",
    "    :param df: DataFrame containing benchmark results.\n",
    "    :param task_name: Name of the task to plot.\n",
    "    :param metrics: List of metrics to plot.\n",
    "    \"\"\"\n",
    "    if df.empty or 'task' not in df.columns: return print(f\"No data for task: {task_name}\")\n",
    "    task_df = df[df['task'] == task_name]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        data = task_df[task_df['full_metric_name'] == metric].dropna(subset=['score'])\n",
    "        if data.empty: \n",
    "            print(f\"Skipping: No valid data for {task_name}/{metric}\")\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.barplot(x='model', y='score', hue='model', data=data, palette=\"viridis\", legend=False)\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f\"{p.get_height():.3f}\", (p.get_x() + p.get_width()/2., p.get_height()), ha='center', va='center', xytext=(0, 9), textcoords='offset points')\n",
    "        plt.title(f\"{task_name} - {metric}\")\n",
    "        plt.xlabel(\"Model\"), plt.ylabel(metric)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout(), plt.show()\n",
    "\n",
    "def plot_metric_across_tasks(df: pd.DataFrame, metric: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot a bar chart for a specific metric across all tasks.\n",
    "    :param df: DataFrame containing benchmark results.\n",
    "    :param metric: Metric to plot.\n",
    "    \"\"\"\n",
    "    data = df[df['full_metric_name'] == metric].dropna(subset=['score'])\n",
    "    if data.empty: return print(f\"No data for metric: {metric}\")\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='task', y='score', hue='model', data=data, palette=\"muted\")\n",
    "    plt.title(f\"{metric} Across Tasks\")\n",
    "    plt.xlabel(\"Task\"), plt.ylabel(f\"{metric} Score\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout(), plt.show()\n",
    "\n",
    "def plot_heatmap_for_metric(df: pd.DataFrame, metric: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot a heatmap for a specific metric across models and tasks.\n",
    "    :param df: DataFrame containing benchmark results.\n",
    "    :param metric: Metric to plot.\n",
    "    \"\"\"\n",
    "    data = df[df['full_metric_name'] == metric].dropna(subset=['score'])\n",
    "    if data.empty: return print(f\"No data for heatmap: {metric}\")\n",
    "\n",
    "    try:\n",
    "        pivot = data.pivot_table(index='model', columns='task', values='score')\n",
    "        if pivot.empty: return print(f\"Empty heatmap for {metric}\")\n",
    "        plt.figure(figsize=(12, max(6, len(pivot)*0.5)))\n",
    "        sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "        plt.title(f\"Heatmap: {metric}\")\n",
    "        plt.xlabel(\"Task\"), plt.ylabel(\"Model\")\n",
    "        plt.xticks(rotation=45, ha=\"right\"), plt.yticks(rotation=0)\n",
    "        plt.tight_layout(), plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in heatmap for {metric}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e45b6",
   "metadata": {},
   "source": [
    "## 4. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None and not results_df.empty and 'full_metric_name' in results_df.columns:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"Generating All Plots\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "    \n",
    "    all_tasks = results_df['task'].unique()\n",
    "    all_metrics = results_df['full_metric_name'].unique()\n",
    "\n",
    "    # --- 1. Bar plots per task ---\n",
    "    for task in all_tasks:\n",
    "        metrics = results_df[results_df['task'] == task]['full_metric_name'].unique()\n",
    "        if metrics.size:\n",
    "            print(f\"\\n--- Task: {task} ---\")\n",
    "            plot_task_metrics(results_df, task, metrics)\n",
    "        else:\n",
    "            print(f\"\\nNo metrics for task '{task}', skipping.\")\n",
    "\n",
    "    # --- 2. Grouped bar plots per metric ---\n",
    "    print(\"\\n--- Plotting each metric across tasks ---\")\n",
    "    for metric in all_metrics:\n",
    "        print(f\"\\nMetric: {metric}\")\n",
    "        plot_metric_across_tasks(results_df, metric)\n",
    "\n",
    "    # --- 3. Heatmaps per metric ---\n",
    "    print(\"\\n--- Generating heatmaps per metric ---\")\n",
    "    for metric in all_metrics:\n",
    "        print(f\"\\nHeatmap for: {metric}\")\n",
    "        plot_heatmap_for_metric(results_df, metric)\n",
    "\n",
    "else:\n",
    "    print(\"Skipping plotting: DataFrame is empty or missing 'full_metric_name'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613cc57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
    "gemma-7b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.3,
            "accuracy": 0.3,
            "f1_score": 0.18502463054187193,
            "precision": 0.2126086956521739,
            "recall": 0.3
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2218583919812811,
                "rouge1_p": 0.2585679370512662,
                "rouge1_r": 0.22231177003674688,
                "rouge2_f": 0.07914960590206192,
                "rouge2_p": 0.08849255934047812,
                "rouge2_r": 0.08582211214204477,
                "rougeL_f": 0.20487498728954823,
                "rougeL_p": 0.237672634926571,
                "rougeL_r": 0.2066709053727606
            },
            "bert_score": {
                "bertscore_precision": 0.811754584312439,
                "bertscore_recall": 0.8114193677902222,
                "bertscore_f1": 0.8109291791915894
            },
            "distinct_ngram": {
                "distinct_1": 0.22073250490516677,
                "distinct_2": 0.4744015957446808,
                "distinct_3": 0.568435282189929
            },
            "word_entropy": 7.8882484040825585,
            "toxicity": {
                "toxic": 0.002736406804760918
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.1690687610167426,
                "rouge1_p": 0.1918410559978149,
                "rouge1_r": 0.21552223486813216,
                "rouge2_f": 0.03860950703718907,
                "rouge2_p": 0.05365138664630009,
                "rouge2_r": 0.05549406715210828,
                "rougeL_f": 0.1546086758446908,
                "rougeL_p": 0.1763498524620539,
                "rougeL_r": 0.19869165016168253,
                "rougeLsum_f": 0.1546086758446908,
                "rougeLsum_p": 0.1763498524620539,
                "rougeLsum_r": 0.19869165016168253
            },
            "bert_score": {
                "bertscore_precision": 0.7275184392929077,
                "bertscore_recall": 0.7378618717193604,
                "bertscore_f1": 0.7323261499404907
            },
            "meteor": 0.11719289829473546,
            "bleu": 0.019131794751963544,
            "distinct_ngram": {
                "distinct_1": 0.37070403737070406,
                "distinct_2": 0.6620386048086692,
                "distinct_3": 0.7422680412371134,
                "distinct_4": 0.7624694802929892
            },
            "word_entropy": 8.611956868467866,
            "semantic_similarity": 0.42572444062680004,
            "toxicity": {
                "toxic": 0.005644258748507127
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.03964551913959262,
            "meteor": 0.2076093820462713,
            "rouge": {
                "rougeL_f": 0.19723536688499663
            },
            "bert_score": {
                "bertscore_precision": 0.5668790936470032,
                "bertscore_recall": 0.6630617380142212,
                "bertscore_f1": 0.6064704656600952
            },
            "semantic_similarity": 0.42489984471350906,
            "toxicity": {
                "toxic": 0.02376148469642082
            }
        }
    },
    "gemma-2b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.24,
            "accuracy": 0.24,
            "f1_score": 0.0929032258064516,
            "precision": 0.0576,
            "recall": 0.24
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2755755661531896,
                "rouge1_p": 0.3733897102035952,
                "rouge1_r": 0.24733339083318143,
                "rouge2_f": 0.08422733335343997,
                "rouge2_p": 0.10710479734788249,
                "rouge2_r": 0.08099098755982227,
                "rougeL_f": 0.2427380284583163,
                "rougeL_p": 0.32738859608913456,
                "rougeL_r": 0.21988687526491948
            },
            "bert_score": {
                "bertscore_precision": 0.8334585428237915,
                "bertscore_recall": 0.8191133141517639,
                "bertscore_f1": 0.8260707259178162
            },
            "distinct_ngram": {
                "distinct_1": 0.18075963404491266,
                "distinct_2": 0.4298566207478212,
                "distinct_3": 0.5470353477765109
            },
            "word_entropy": 7.823374043716019,
            "toxicity": {
                "toxic": 0.004777482938952744
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.20581408091548314,
                "rouge1_p": 0.17424403370021171,
                "rouge1_r": 0.33744723987721037,
                "rouge2_f": 0.06506391623813101,
                "rouge2_p": 0.047307237954094736,
                "rouge2_r": 0.140867491896734,
                "rougeL_f": 0.1877789233704643,
                "rougeL_p": 0.15920375065181389,
                "rougeL_r": 0.3096921677941297,
                "rougeLsum_f": 0.1877789233704643,
                "rougeLsum_p": 0.15920375065181389,
                "rougeLsum_r": 0.3096921677941297
            },
            "bert_score": {
                "bertscore_precision": 0.8136962056159973,
                "bertscore_recall": 0.8521115183830261,
                "bertscore_f1": 0.8321948051452637
            },
            "meteor": 0.19529346976313114,
            "bleu": 0.030732838281682984,
            "distinct_ngram": {
                "distinct_1": 0.3116009714178965,
                "distinct_2": 0.5777861587780502,
                "distinct_3": 0.6525794783932991,
                "distinct_4": 0.673842014222564
            },
            "word_entropy": 8.888305369133999,
            "semantic_similarity": 0.5237745118141174,
            "toxicity": {
                "toxic": 0.0056865393824409695
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.02238707611787973,
            "meteor": 0.10088965398056553,
            "rouge": {
                "rougeL_f": 0.09031604284774707
            },
            "bert_score": {
                "bertscore_precision": 0.5273199677467346,
                "bertscore_recall": 0.6345043778419495,
                "bertscore_f1": 0.5749650001525879
            },
            "semantic_similarity": 0.26054286658763887,
            "toxicity": {
                "toxic": 0.010815110247349366
            }
        }
    },
    "gpt2 full prec": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.24,
            "accuracy": 0.24,
            "f1_score": 0.0929032258064516,
            "precision": 0.0576,
            "recall": 0.24
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2799513986456262,
                "rouge1_p": 0.3242927247285401,
                "rouge1_r": 0.266692957949462,
                "rouge2_f": 0.09413179444027168,
                "rouge2_p": 0.11439618641326338,
                "rouge2_r": 0.08768913521455465,
                "rougeL_f": 0.25422921765317114,
                "rougeL_p": 0.29417849832405346,
                "rougeL_r": 0.24322678487446775
            },
            "bert_score": {
                "bertscore_precision": 0.8213069438934326,
                "bertscore_recall": 0.7957189083099365,
                "bertscore_f1": 0.808066189289093
            },
            "distinct_ngram": {
                "distinct_1": 0.1372707115500907,
                "distinct_2": 0.3101201384646711,
                "distinct_3": 0.36350545155317837
            },
            "word_entropy": 8.30228669130595,
            "toxicity": {
                "toxic": 0.004571489989757538
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.1952999472397369,
                "rouge1_p": 0.17738144426687888,
                "rouge1_r": 0.27969921241864754,
                "rouge2_f": 0.05561029158406275,
                "rouge2_p": 0.042775919902146076,
                "rouge2_r": 0.11032705527951805,
                "rougeL_f": 0.17519282407291598,
                "rougeL_p": 0.15743261473751466,
                "rougeL_r": 0.2546164470266072,
                "rougeLsum_f": 0.17519282407291598,
                "rougeLsum_p": 0.15743261473751466,
                "rougeLsum_r": 0.2546164470266072
            },
            "bert_score": {
                "bertscore_precision": 0.7990107536315918,
                "bertscore_recall": 0.8402841091156006,
                "bertscore_f1": 0.8189771771430969
            },
            "meteor": 0.1662910859947659,
            "bleu": 0.022140728460785786,
            "distinct_ngram": {
                "distinct_1": 0.22164048865619546,
                "distinct_2": 0.4119718309859155,
                "distinct_3": 0.4674955595026643,
                "distinct_4": 0.4854838709677419
            },
            "word_entropy": 8.527699837709825,
            "semantic_similarity": 0.4282629035413265,
            "toxicity": {
                "toxic": 0.005224728328175843
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.0060875342877561205,
            "meteor": 0.03639975963616509,
            "rouge": {
                "rougeL_f": 0.07498771802126525
            },
            "bert_score": {
                "bertscore_precision": 0.5424299836158752,
                "bertscore_recall": 0.597045361995697,
                "bertscore_f1": 0.5677289366722107
            },
            "semantic_similarity": 0.5532233974337578,
            "toxicity": {
                "toxic": 0.033763069207780064
            }
        }
    },
    "phi-2 - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.48,
            "accuracy": 0.48,
            "f1_score": 0.46337634627108315,
            "precision": 0.5121904761904762,
            "recall": 0.48
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.3526705318405746,
                "rouge1_p": 0.32662149097542553,
                "rouge1_r": 0.42840192135216154,
                "rouge2_f": 0.13976116005230257,
                "rouge2_p": 0.12324136330818938,
                "rouge2_r": 0.1835123994530485,
                "rougeL_f": 0.3160549696011339,
                "rougeL_p": 0.2927549318152352,
                "rougeL_r": 0.3851816110250938
            },
            "bert_score": {
                "bertscore_precision": 0.8512676358222961,
                "bertscore_recall": 0.8588039875030518,
                "bertscore_f1": 0.8549391031265259
            },
            "distinct_ngram": {
                "distinct_1": 0.20007995202878273,
                "distinct_2": 0.5515848980415909,
                "distinct_3": 0.7489292270038752
            },
            "word_entropy": 8.134404006316489,
            "toxicity": {
                "toxic": 0.0010582017549313604
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.2326043861593088,
                "rouge1_p": 0.17357565906244157,
                "rouge1_r": 0.41111447868820966,
                "rouge2_f": 0.07101441574665221,
                "rouge2_p": 0.04898015997717109,
                "rouge2_r": 0.15084603835465063,
                "rougeL_f": 0.21762416750447752,
                "rougeL_p": 0.1624422730015243,
                "rougeL_r": 0.3848970314699096,
                "rougeLsum_f": 0.21762416750447752,
                "rougeLsum_p": 0.1624422730015243,
                "rougeLsum_r": 0.3848970314699096
            },
            "bert_score": {
                "bertscore_precision": 0.8316993117332458,
                "bertscore_recall": 0.8680868744850159,
                "bertscore_f1": 0.8493511080741882
            },
            "meteor": 0.2425453930665446,
            "bleu": 0.028297245985432713,
            "distinct_ngram": {
                "distinct_1": 0.36320235034887993,
                "distinct_2": 0.7207190511489993,
                "distinct_3": 0.82996632996633,
                "distinct_4": 0.8538519637462235
            },
            "word_entropy": 9.16696311116288,
            "semantic_similarity": 0.6200586718320846,
            "toxicity": {
                "toxic": 0.015874030914856122
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.02503606965560283,
            "meteor": 0.16379983101423876,
            "rouge": {
                "rougeL_f": 0.14769063217319786
            },
            "bert_score": {
                "bertscore_precision": 0.5845333933830261,
                "bertscore_recall": 0.6879999041557312,
                "bertscore_f1": 0.6308208107948303
            },
            "semantic_similarity": 0.4533509338647127,
            "toxicity": {
                "toxic": 0.005724779781885445
            }
        }
    },
    "mistral-7b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.32,
            "accuracy": 0.32,
            "f1_score": 0.2790526315789474,
            "precision": 0.5298245614035088,
            "recall": 0.32
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.23593590949531681,
                "rouge1_p": 0.2622887802884216,
                "rouge1_r": 0.23654824005355451,
                "rouge2_f": 0.07556850971982566,
                "rouge2_p": 0.08036920510065788,
                "rouge2_r": 0.08032075283860478,
                "rougeL_f": 0.21655285532360377,
                "rougeL_p": 0.24051067871676035,
                "rougeL_r": 0.2181319972653124
            },
            "bert_score": {
                "bertscore_precision": 0.8169784545898438,
                "bertscore_recall": 0.801907479763031,
                "bertscore_f1": 0.8092057704925537
            },
            "distinct_ngram": {
                "distinct_1": 0.17446043165467626,
                "distinct_2": 0.42243623112962,
                "distinct_3": 0.5627637130801688
            },
            "word_entropy": 7.901374154477449,
            "toxicity": {
                "toxic": 0.0018432492937427013
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.19294341481606378,
                "rouge1_p": 0.15988536686503704,
                "rouge1_r": 0.29913959264516915,
                "rouge2_f": 0.045705180957056224,
                "rouge2_p": 0.030971505119112085,
                "rouge2_r": 0.09843837010920334,
                "rougeL_f": 0.1788575936796168,
                "rougeL_p": 0.14685237430742662,
                "rougeL_r": 0.27992775233708567,
                "rougeLsum_f": 0.1788575936796168,
                "rougeLsum_p": 0.14685237430742662,
                "rougeLsum_r": 0.27992775233708567
            },
            "bert_score": {
                "bertscore_precision": 0.8053221106529236,
                "bertscore_recall": 0.8331478834152222,
                "bertscore_f1": 0.8188288807868958
            },
            "meteor": 0.1743417986892266,
            "bleu": 0.026212611620810677,
            "distinct_ngram": {
                "distinct_1": 0.36532882011605416,
                "distinct_2": 0.6848544164423783,
                "distinct_3": 0.7815750371471025,
                "distinct_4": 0.8109801955377287
            },
            "word_entropy": 8.95786426174044,
            "semantic_similarity": 0.5257429894804955,
            "toxicity": {
                "toxic": 0.02607820171979256
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.04402438318122554,
            "meteor": 0.24263082004258837,
            "rouge": {
                "rougeL_f": 0.25286900763458
            },
            "bert_score": {
                "bertscore_precision": 0.5734155774116516,
                "bertscore_recall": 0.6815169453620911,
                "bertscore_f1": 0.6211054921150208
            },
            "semantic_similarity": 0.4295930951833725,
            "toxicity": {
                "toxic": 0.009263102253898979
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "mistral-7b - 8quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.48,
            "accuracy": 0.48,
            "f1_score": 0.45117408906882595,
            "precision": 0.513968253968254,
            "recall": 0.48
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.26845968659647373,
                "rouge1_p": 0.286455496001664,
                "rouge1_r": 0.282770039216403,
                "rouge2_f": 0.08765385876481023,
                "rouge2_p": 0.08966333907497946,
                "rouge2_r": 0.09439873506785638,
                "rougeL_f": 0.2440811722076377,
                "rougeL_p": 0.26036372036244987,
                "rougeL_r": 0.2560279516484198
            },
            "bert_score": {
                "bertscore_precision": 0.825437605381012,
                "bertscore_recall": 0.8102545738220215,
                "bertscore_f1": 0.817545473575592
            },
            "distinct_ngram": {
                "distinct_1": 0.19069412662090007,
                "distinct_2": 0.4540303888745815,
                "distinct_3": 0.5710931385337855
            },
            "word_entropy": 8.127717935361044,
            "toxicity": {
                "toxic": 0.0020290016185026617
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.21197534833099266,
                "rouge1_p": 0.17045055692837027,
                "rouge1_r": 0.3179114406103287,
                "rouge2_f": 0.058671123928458954,
                "rouge2_p": 0.04263283954958018,
                "rouge2_r": 0.10378804657203151,
                "rougeL_f": 0.18914798946231431,
                "rougeL_p": 0.1527858339759541,
                "rougeL_r": 0.2835204192818074,
                "rougeLsum_f": 0.18914798946231431,
                "rougeLsum_p": 0.1527858339759541,
                "rougeLsum_r": 0.2835204192818074
            },
            "bert_score": {
                "bertscore_precision": 0.8293972015380859,
                "bertscore_recall": 0.852491557598114,
                "bertscore_f1": 0.8406503796577454
            },
            "meteor": 0.19350330350680772,
            "bleu": 0.024393203585213682,
            "distinct_ngram": {
                "distinct_1": 0.4238563983786914,
                "distinct_2": 0.7258347978910369,
                "distinct_3": 0.8097213989330172,
                "distinct_4": 0.8344331133773245
            },
            "word_entropy": 8.23231648157632,
            "semantic_similarity": 0.5633894518017769,
            "toxicity": {
                "toxic": 0.003490350834908895
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.04621584655755272,
            "meteor": 0.25807893937323634,
            "rouge": {
                "rougeL_f": 0.23764459199153104
            },
            "bert_score": {
                "bertscore_precision": 0.5682207942008972,
                "bertscore_recall": 0.6940007209777832,
                "bertscore_f1": 0.6231771111488342
            },
            "semantic_similarity": 0.4527017419040203,
            "toxicity": {
                "toxic": 0.0108533993747551
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "llama3-8b - 8quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.44,
            "accuracy": 0.44,
            "f1_score": 0.4326412870119964,
            "precision": 0.5135353535353535,
            "recall": 0.44
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.35565978300650763,
                "rouge1_p": 0.3815281605267142,
                "rouge1_r": 0.35242840127199476,
                "rouge2_f": 0.12329126687275525,
                "rouge2_p": 0.1254028673944935,
                "rouge2_r": 0.12966228733598276,
                "rougeL_f": 0.320852883621099,
                "rougeL_p": 0.3430268435124324,
                "rougeL_r": 0.31907950409562863
            },
            "bert_score": {
                "bertscore_precision": 0.8208271861076355,
                "bertscore_recall": 0.8036942481994629,
                "bertscore_f1": 0.8119741082191467
            },
            "distinct_ngram": {
                "distinct_1": 0.22659713168187745,
                "distinct_2": 0.5320834433588593,
                "distinct_3": 0.6616742444503878
            },
            "word_entropy": 8.313358051062995,
            "toxicity": {
                "toxic": 0.0015181063208729029
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.21680051790230145,
                "rouge1_p": 0.1705761196029087,
                "rouge1_r": 0.37155286691960715,
                "rouge2_f": 0.0628123048496326,
                "rouge2_p": 0.043073136777542466,
                "rouge2_r": 0.14316199623155484,
                "rougeL_f": 0.20127878616337805,
                "rougeL_p": 0.15792540936979013,
                "rougeL_r": 0.3467605750591625,
                "rougeLsum_f": 0.20127878616337805,
                "rougeLsum_p": 0.15792540936979013,
                "rougeLsum_r": 0.3467605750591625
            },
            "bert_score": {
                "bertscore_precision": 0.8061001300811768,
                "bertscore_recall": 0.8419538736343384,
                "bertscore_f1": 0.8234115839004517
            },
            "meteor": 0.21212032478709097,
            "bleu": 0.034758945905171715,
            "distinct_ngram": {
                "distinct_1": 0.36408147277712494,
                "distinct_2": 0.6820249159580779,
                "distinct_3": 0.7735623003194888,
                "distinct_4": 0.7935067553942327
            },
            "word_entropy": 9.137783757476786,
            "semantic_similarity": 0.5317982311546803,
            "toxicity": {
                "toxic": 0.0022783192817587405
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.05202283722659722,
            "meteor": 0.2586628092089009,
            "rouge": {
                "rougeL_f": 0.2500611705571852
            },
            "bert_score": {
                "bertscore_precision": 0.592379093170166,
                "bertscore_recall": 0.6853244304656982,
                "bertscore_f1": 0.6334099173545837
            },
            "semantic_similarity": 0.531291221678257,
            "toxicity": {
                "toxic": 0.029045576199423523
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.58,
            "f1_score": 0.7272727272727273,
            "precision": 0.5957446808510638,
            "recall": 0.9333333333333333
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "llama2-7b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.34,
            "accuracy": 0.34,
            "f1_score": 0.3001449275362319,
            "precision": 0.3965367965367965,
            "recall": 0.34
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2142269377858024,
                "rouge1_p": 0.2768809539371684,
                "rouge1_r": 0.1943505387101138,
                "rouge2_f": 0.0671292704944741,
                "rouge2_p": 0.08487825100559,
                "rouge2_r": 0.06084399195548089,
                "rougeL_f": 0.1969015401333094,
                "rougeL_p": 0.2544660493972638,
                "rougeL_r": 0.17939002829976697
            },
            "bert_score": {
                "bertscore_precision": 0.7963831424713135,
                "bertscore_recall": 0.7952633500099182,
                "bertscore_f1": 0.7954913377761841
            },
            "distinct_ngram": {
                "distinct_1": 0.17480359147025815,
                "distinct_2": 0.3867387592487194,
                "distinct_3": 0.45640877598152424
            },
            "word_entropy": 8.016015980938764,
            "toxicity": {
                "toxic": 0.005686844593146816
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.22459221852115333,
                "rouge1_p": 0.1747198200855967,
                "rouge1_r": 0.36492718005923397,
                "rouge2_f": 0.06957941968614635,
                "rouge2_p": 0.04824865894020953,
                "rouge2_r": 0.14276203141507607,
                "rougeL_f": 0.20700361312165494,
                "rougeL_p": 0.1605963814088762,
                "rougeL_r": 0.33847457177376344,
                "rougeLsum_f": 0.20700361312165494,
                "rougeLsum_p": 0.1605963814088762,
                "rougeLsum_r": 0.33847457177376344
            },
            "bert_score": {
                "bertscore_precision": 0.8044140338897705,
                "bertscore_recall": 0.8388946652412415,
                "bertscore_f1": 0.8211774230003357
            },
            "meteor": 0.2091339422012403,
            "bleu": 0.035279456189579686,
            "distinct_ngram": {
                "distinct_1": 0.36508273228680527,
                "distinct_2": 0.6840300107181136,
                "distinct_3": 0.7816291161178509,
                "distinct_4": 0.8105977665863806
            },
            "word_entropy": 9.077406575931672,
            "semantic_similarity": 0.5507720416039228,
            "toxicity": {
                "toxic": 0.013780669302213937
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.1339538078970193,
            "meteor": 0.3065956505145932,
            "rouge": {
                "rougeL_f": 0.33545900395390504
            },
            "bert_score": {
                "bertscore_precision": 0.7748383283615112,
                "bertscore_recall": 0.8016892075538635,
                "bertscore_f1": 0.7870602607727051
            },
            "semantic_similarity": 0.7418132495880126,
            "toxicity": {
                "toxic": 0.003410458113066852
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "llama3-8b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.26,
            "accuracy": 0.26,
            "f1_score": 0.2058722976370035,
            "precision": 0.37666666666666665,
            "recall": 0.26
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.3463624670033557,
                "rouge1_p": 0.4313593540354754,
                "rouge1_r": 0.3251806628129426,
                "rouge2_f": 0.1318792163688263,
                "rouge2_p": 0.1741848414214286,
                "rouge2_r": 0.12981274730117512,
                "rougeL_f": 0.3121058662126471,
                "rougeL_p": 0.3828409389200964,
                "rougeL_r": 0.2937063951147287
            },
            "bert_score": {
                "bertscore_precision": 0.7389988899230957,
                "bertscore_recall": 0.7114666700363159,
                "bertscore_f1": 0.7246501445770264
            },
            "distinct_ngram": {
                "distinct_1": 0.21654875422946784,
                "distinct_2": 0.48659600997506236,
                "distinct_3": 0.5930489731437599
            },
            "word_entropy": 8.284307561015252,
            "toxicity": {
                "toxic": 0.002392488119658083
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.21094067365289376,
                "rouge1_p": 0.16847242250799233,
                "rouge1_r": 0.34944108591266565,
                "rouge2_f": 0.064767025875799,
                "rouge2_p": 0.04546235482163479,
                "rouge2_r": 0.1368926202818165,
                "rougeL_f": 0.19675173752965733,
                "rougeL_p": 0.1562300742254482,
                "rougeL_r": 0.328734328936701,
                "rougeLsum_f": 0.19675173752965733,
                "rougeLsum_p": 0.1562300742254482,
                "rougeLsum_r": 0.328734328936701
            },
            "bert_score": {
                "bertscore_precision": 0.8211895823478699,
                "bertscore_recall": 0.8564806580543518,
                "bertscore_f1": 0.8382613658905029
            },
            "meteor": 0.20670972553506647,
            "bleu": 0.033007635928418995,
            "distinct_ngram": {
                "distinct_1": 0.3374164810690423,
                "distinct_2": 0.6315099288122893,
                "distinct_3": 0.7186081694402421,
                "distinct_4": 0.7361588392516227
            },
            "word_entropy": 9.05348887887659,
            "semantic_similarity": 0.5516497104242444,
            "toxicity": {
                "toxic": 0.0028006199887022376
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.07668806911718518,
            "meteor": 0.2947626267085073,
            "rouge": {
                "rougeL_f": 0.3060470613135573
            },
            "bert_score": {
                "bertscore_precision": 0.68413245677948,
                "bertscore_recall": 0.7479808926582336,
                "bertscore_f1": 0.7120704650878906
            },
            "semantic_similarity": 0.638110869973898,
            "toxicity": {
                "toxic": 0.015429323447169735
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.4230769230769231,
            "precision": 0.5,
            "recall": 0.36666666666666664
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    }
}
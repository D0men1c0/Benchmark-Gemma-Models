{
    "gemma-7b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.3,
            "accuracy": 0.3,
            "f1_score": 0.18502463054187193,
            "precision": 0.2126086956521739,
            "recall": 0.3
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2218583919812811,
                "rouge1_p": 0.2585679370512662,
                "rouge1_r": 0.22231177003674688,
                "rouge2_f": 0.07914960590206192,
                "rouge2_p": 0.08849255934047812,
                "rouge2_r": 0.08582211214204477,
                "rougeL_f": 0.20487498728954823,
                "rougeL_p": 0.237672634926571,
                "rougeL_r": 0.2066709053727606
            },
            "bert_score": {
                "bertscore_precision": 0.811754584312439,
                "bertscore_recall": 0.8114193677902222,
                "bertscore_f1": 0.8109291791915894
            },
            "distinct_ngram": {
                "distinct_1": 0.22073250490516677,
                "distinct_2": 0.4744015957446808,
                "distinct_3": 0.568435282189929
            },
            "word_entropy": 7.8882484040825585,
            "toxicity": {
                "toxic": 0.002736406804760918
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.1690687610167426,
                "rouge1_p": 0.1918410559978149,
                "rouge1_r": 0.21552223486813216,
                "rouge2_f": 0.03860950703718907,
                "rouge2_p": 0.05365138664630009,
                "rouge2_r": 0.05549406715210828,
                "rougeL_f": 0.1546086758446908,
                "rougeL_p": 0.1763498524620539,
                "rougeL_r": 0.19869165016168253,
                "rougeLsum_f": 0.1546086758446908,
                "rougeLsum_p": 0.1763498524620539,
                "rougeLsum_r": 0.19869165016168253
            },
            "bert_score": {
                "bertscore_precision": 0.7275184392929077,
                "bertscore_recall": 0.7378618717193604,
                "bertscore_f1": 0.7323261499404907
            },
            "meteor": 0.11719289829473546,
            "bleu": 0.019131794751963544,
            "distinct_ngram": {
                "distinct_1": 0.37070403737070406,
                "distinct_2": 0.6620386048086692,
                "distinct_3": 0.7422680412371134,
                "distinct_4": 0.7624694802929892
            },
            "word_entropy": 8.611956868467866,
            "semantic_similarity": 0.42572444062680004,
            "toxicity": {
                "toxic": 0.005644258748507127
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.03964551913959262,
            "meteor": 0.2076093820462713,
            "rouge": {
                "rougeL_f": 0.19723536688499663
            },
            "bert_score": {
                "bertscore_precision": 0.5668790936470032,
                "bertscore_recall": 0.6630617380142212,
                "bertscore_f1": 0.6064704656600952
            },
            "semantic_similarity": 0.42489984471350906,
            "toxicity": {
                "toxic": 0.02376148469642082
            }
        }
    },
    "gemma-2b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.24,
            "accuracy": 0.24,
            "f1_score": 0.0929032258064516,
            "precision": 0.0576,
            "recall": 0.24
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2755755661531896,
                "rouge1_p": 0.3733897102035952,
                "rouge1_r": 0.24733339083318143,
                "rouge2_f": 0.08422733335343997,
                "rouge2_p": 0.10710479734788249,
                "rouge2_r": 0.08099098755982227,
                "rougeL_f": 0.2427380284583163,
                "rougeL_p": 0.32738859608913456,
                "rougeL_r": 0.21988687526491948
            },
            "bert_score": {
                "bertscore_precision": 0.8334585428237915,
                "bertscore_recall": 0.8191133141517639,
                "bertscore_f1": 0.8260707259178162
            },
            "distinct_ngram": {
                "distinct_1": 0.18075963404491266,
                "distinct_2": 0.4298566207478212,
                "distinct_3": 0.5470353477765109
            },
            "word_entropy": 7.823374043716019,
            "toxicity": {
                "toxic": 0.004777482938952744
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.20581408091548314,
                "rouge1_p": 0.17424403370021171,
                "rouge1_r": 0.33744723987721037,
                "rouge2_f": 0.06506391623813101,
                "rouge2_p": 0.047307237954094736,
                "rouge2_r": 0.140867491896734,
                "rougeL_f": 0.1877789233704643,
                "rougeL_p": 0.15920375065181389,
                "rougeL_r": 0.3096921677941297,
                "rougeLsum_f": 0.1877789233704643,
                "rougeLsum_p": 0.15920375065181389,
                "rougeLsum_r": 0.3096921677941297
            },
            "bert_score": {
                "bertscore_precision": 0.8136962056159973,
                "bertscore_recall": 0.8521115183830261,
                "bertscore_f1": 0.8321948051452637
            },
            "meteor": 0.19529346976313114,
            "bleu": 0.030732838281682984,
            "distinct_ngram": {
                "distinct_1": 0.3116009714178965,
                "distinct_2": 0.5777861587780502,
                "distinct_3": 0.6525794783932991,
                "distinct_4": 0.673842014222564
            },
            "word_entropy": 8.888305369133999,
            "semantic_similarity": 0.5237745118141174,
            "toxicity": {
                "toxic": 0.0056865393824409695
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.02238707611787973,
            "meteor": 0.10088965398056553,
            "rouge": {
                "rougeL_f": 0.09031604284774707
            },
            "bert_score": {
                "bertscore_precision": 0.5273199677467346,
                "bertscore_recall": 0.6345043778419495,
                "bertscore_f1": 0.5749650001525879
            },
            "semantic_similarity": 0.26054286658763887,
            "toxicity": {
                "toxic": 0.010815110247349366
            }
        }
    },
    "gpt2 full prec": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.24,
            "accuracy": 0.24,
            "f1_score": 0.0929032258064516,
            "precision": 0.0576,
            "recall": 0.24
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2799513986456262,
                "rouge1_p": 0.3242927247285401,
                "rouge1_r": 0.266692957949462,
                "rouge2_f": 0.09413179444027168,
                "rouge2_p": 0.11439618641326338,
                "rouge2_r": 0.08768913521455465,
                "rougeL_f": 0.25422921765317114,
                "rougeL_p": 0.29417849832405346,
                "rougeL_r": 0.24322678487446775
            },
            "bert_score": {
                "bertscore_precision": 0.8213069438934326,
                "bertscore_recall": 0.7957189083099365,
                "bertscore_f1": 0.808066189289093
            },
            "distinct_ngram": {
                "distinct_1": 0.1372707115500907,
                "distinct_2": 0.3101201384646711,
                "distinct_3": 0.36350545155317837
            },
            "word_entropy": 8.30228669130595,
            "toxicity": {
                "toxic": 0.004571489989757538
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.1952999472397369,
                "rouge1_p": 0.17738144426687888,
                "rouge1_r": 0.27969921241864754,
                "rouge2_f": 0.05561029158406275,
                "rouge2_p": 0.042775919902146076,
                "rouge2_r": 0.11032705527951805,
                "rougeL_f": 0.17519282407291598,
                "rougeL_p": 0.15743261473751466,
                "rougeL_r": 0.2546164470266072,
                "rougeLsum_f": 0.17519282407291598,
                "rougeLsum_p": 0.15743261473751466,
                "rougeLsum_r": 0.2546164470266072
            },
            "bert_score": {
                "bertscore_precision": 0.7990107536315918,
                "bertscore_recall": 0.8402841091156006,
                "bertscore_f1": 0.8189771771430969
            },
            "meteor": 0.1662910859947659,
            "bleu": 0.022140728460785786,
            "distinct_ngram": {
                "distinct_1": 0.22164048865619546,
                "distinct_2": 0.4119718309859155,
                "distinct_3": 0.4674955595026643,
                "distinct_4": 0.4854838709677419
            },
            "word_entropy": 8.527699837709825,
            "semantic_similarity": 0.4282629035413265,
            "toxicity": {
                "toxic": 0.005224728328175843
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.0060875342877561205,
            "meteor": 0.03639975963616509,
            "rouge": {
                "rougeL_f": 0.07498771802126525
            },
            "bert_score": {
                "bertscore_precision": 0.5424299836158752,
                "bertscore_recall": 0.597045361995697,
                "bertscore_f1": 0.5677289366722107
            },
            "semantic_similarity": 0.5532233974337578,
            "toxicity": {
                "toxic": 0.033763069207780064
            }
        }
    },
    "phi-2 - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.48,
            "accuracy": 0.48,
            "f1_score": 0.46337634627108315,
            "precision": 0.5121904761904762,
            "recall": 0.48
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.3526705318405746,
                "rouge1_p": 0.32662149097542553,
                "rouge1_r": 0.42840192135216154,
                "rouge2_f": 0.13976116005230257,
                "rouge2_p": 0.12324136330818938,
                "rouge2_r": 0.1835123994530485,
                "rougeL_f": 0.3160549696011339,
                "rougeL_p": 0.2927549318152352,
                "rougeL_r": 0.3851816110250938
            },
            "bert_score": {
                "bertscore_precision": 0.8512676358222961,
                "bertscore_recall": 0.8588039875030518,
                "bertscore_f1": 0.8549391031265259
            },
            "distinct_ngram": {
                "distinct_1": 0.20007995202878273,
                "distinct_2": 0.5515848980415909,
                "distinct_3": 0.7489292270038752
            },
            "word_entropy": 8.134404006316489,
            "toxicity": {
                "toxic": 0.0010582017549313604
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.2326043861593088,
                "rouge1_p": 0.17357565906244157,
                "rouge1_r": 0.41111447868820966,
                "rouge2_f": 0.07101441574665221,
                "rouge2_p": 0.04898015997717109,
                "rouge2_r": 0.15084603835465063,
                "rougeL_f": 0.21762416750447752,
                "rougeL_p": 0.1624422730015243,
                "rougeL_r": 0.3848970314699096,
                "rougeLsum_f": 0.21762416750447752,
                "rougeLsum_p": 0.1624422730015243,
                "rougeLsum_r": 0.3848970314699096
            },
            "bert_score": {
                "bertscore_precision": 0.8316993117332458,
                "bertscore_recall": 0.8680868744850159,
                "bertscore_f1": 0.8493511080741882
            },
            "meteor": 0.2425453930665446,
            "bleu": 0.028297245985432713,
            "distinct_ngram": {
                "distinct_1": 0.36320235034887993,
                "distinct_2": 0.7207190511489993,
                "distinct_3": 0.82996632996633,
                "distinct_4": 0.8538519637462235
            },
            "word_entropy": 9.16696311116288,
            "semantic_similarity": 0.6200586718320846,
            "toxicity": {
                "toxic": 0.015874030914856122
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.02503606965560283,
            "meteor": 0.16379983101423876,
            "rouge": {
                "rougeL_f": 0.14769063217319786
            },
            "bert_score": {
                "bertscore_precision": 0.5845333933830261,
                "bertscore_recall": 0.6879999041557312,
                "bertscore_f1": 0.6308208107948303
            },
            "semantic_similarity": 0.4533509338647127,
            "toxicity": {
                "toxic": 0.005724779781885445
            }
        }
    },
    "mistral-7b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.32,
            "accuracy": 0.32,
            "f1_score": 0.2790526315789474,
            "precision": 0.5298245614035088,
            "recall": 0.32
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.23593590949531681,
                "rouge1_p": 0.2622887802884216,
                "rouge1_r": 0.23654824005355451,
                "rouge2_f": 0.07556850971982566,
                "rouge2_p": 0.08036920510065788,
                "rouge2_r": 0.08032075283860478,
                "rougeL_f": 0.21655285532360377,
                "rougeL_p": 0.24051067871676035,
                "rougeL_r": 0.2181319972653124
            },
            "bert_score": {
                "bertscore_precision": 0.8169784545898438,
                "bertscore_recall": 0.801907479763031,
                "bertscore_f1": 0.8092057704925537
            },
            "distinct_ngram": {
                "distinct_1": 0.17446043165467626,
                "distinct_2": 0.42243623112962,
                "distinct_3": 0.5627637130801688
            },
            "word_entropy": 7.901374154477449,
            "toxicity": {
                "toxic": 0.0018432492937427013
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.19294341481606378,
                "rouge1_p": 0.15988536686503704,
                "rouge1_r": 0.29913959264516915,
                "rouge2_f": 0.045705180957056224,
                "rouge2_p": 0.030971505119112085,
                "rouge2_r": 0.09843837010920334,
                "rougeL_f": 0.1788575936796168,
                "rougeL_p": 0.14685237430742662,
                "rougeL_r": 0.27992775233708567,
                "rougeLsum_f": 0.1788575936796168,
                "rougeLsum_p": 0.14685237430742662,
                "rougeLsum_r": 0.27992775233708567
            },
            "bert_score": {
                "bertscore_precision": 0.8053221106529236,
                "bertscore_recall": 0.8331478834152222,
                "bertscore_f1": 0.8188288807868958
            },
            "meteor": 0.1743417986892266,
            "bleu": 0.026212611620810677,
            "distinct_ngram": {
                "distinct_1": 0.36532882011605416,
                "distinct_2": 0.6848544164423783,
                "distinct_3": 0.7815750371471025,
                "distinct_4": 0.8109801955377287
            },
            "word_entropy": 8.95786426174044,
            "semantic_similarity": 0.5257429894804955,
            "toxicity": {
                "toxic": 0.02607820171979256
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.04402438318122554,
            "meteor": 0.24263082004258837,
            "rouge": {
                "rougeL_f": 0.25286900763458
            },
            "bert_score": {
                "bertscore_precision": 0.5734155774116516,
                "bertscore_recall": 0.6815169453620911,
                "bertscore_f1": 0.6211054921150208
            },
            "semantic_similarity": 0.4295930951833725,
            "toxicity": {
                "toxic": 0.009263102253898979
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "mistral-7b - 8quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.48,
            "accuracy": 0.48,
            "f1_score": 0.45117408906882595,
            "precision": 0.513968253968254,
            "recall": 0.48
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.26845968659647373,
                "rouge1_p": 0.286455496001664,
                "rouge1_r": 0.282770039216403,
                "rouge2_f": 0.08765385876481023,
                "rouge2_p": 0.08966333907497946,
                "rouge2_r": 0.09439873506785638,
                "rougeL_f": 0.2440811722076377,
                "rougeL_p": 0.26036372036244987,
                "rougeL_r": 0.2560279516484198
            },
            "bert_score": {
                "bertscore_precision": 0.825437605381012,
                "bertscore_recall": 0.8102545738220215,
                "bertscore_f1": 0.817545473575592
            },
            "distinct_ngram": {
                "distinct_1": 0.19069412662090007,
                "distinct_2": 0.4540303888745815,
                "distinct_3": 0.5710931385337855
            },
            "word_entropy": 8.127717935361044,
            "toxicity": {
                "toxic": 0.0020290016185026617
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.21197534833099266,
                "rouge1_p": 0.17045055692837027,
                "rouge1_r": 0.3179114406103287,
                "rouge2_f": 0.058671123928458954,
                "rouge2_p": 0.04263283954958018,
                "rouge2_r": 0.10378804657203151,
                "rougeL_f": 0.18914798946231431,
                "rougeL_p": 0.1527858339759541,
                "rougeL_r": 0.2835204192818074,
                "rougeLsum_f": 0.18914798946231431,
                "rougeLsum_p": 0.1527858339759541,
                "rougeLsum_r": 0.2835204192818074
            },
            "bert_score": {
                "bertscore_precision": 0.8293972015380859,
                "bertscore_recall": 0.852491557598114,
                "bertscore_f1": 0.8406503796577454
            },
            "meteor": 0.19350330350680772,
            "bleu": 0.024393203585213682,
            "distinct_ngram": {
                "distinct_1": 0.4238563983786914,
                "distinct_2": 0.7258347978910369,
                "distinct_3": 0.8097213989330172,
                "distinct_4": 0.8344331133773245
            },
            "word_entropy": 8.23231648157632,
            "semantic_similarity": 0.5633894518017769,
            "toxicity": {
                "toxic": 0.003490350834908895
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.04621584655755272,
            "meteor": 0.25807893937323634,
            "rouge": {
                "rougeL_f": 0.23764459199153104
            },
            "bert_score": {
                "bertscore_precision": 0.5682207942008972,
                "bertscore_recall": 0.6940007209777832,
                "bertscore_f1": 0.6231771111488342
            },
            "semantic_similarity": 0.4527017419040203,
            "toxicity": {
                "toxic": 0.0108533993747551
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "llama3-8b - 8quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.44,
            "accuracy": 0.44,
            "f1_score": 0.4326412870119964,
            "precision": 0.5135353535353535,
            "recall": 0.44
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.35565978300650763,
                "rouge1_p": 0.3815281605267142,
                "rouge1_r": 0.35242840127199476,
                "rouge2_f": 0.12329126687275525,
                "rouge2_p": 0.1254028673944935,
                "rouge2_r": 0.12966228733598276,
                "rougeL_f": 0.320852883621099,
                "rougeL_p": 0.3430268435124324,
                "rougeL_r": 0.31907950409562863
            },
            "bert_score": {
                "bertscore_precision": 0.8208271861076355,
                "bertscore_recall": 0.8036942481994629,
                "bertscore_f1": 0.8119741082191467
            },
            "distinct_ngram": {
                "distinct_1": 0.22659713168187745,
                "distinct_2": 0.5320834433588593,
                "distinct_3": 0.6616742444503878
            },
            "word_entropy": 8.313358051062995,
            "toxicity": {
                "toxic": 0.0015181063208729029
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.21680051790230145,
                "rouge1_p": 0.1705761196029087,
                "rouge1_r": 0.37155286691960715,
                "rouge2_f": 0.0628123048496326,
                "rouge2_p": 0.043073136777542466,
                "rouge2_r": 0.14316199623155484,
                "rougeL_f": 0.20127878616337805,
                "rougeL_p": 0.15792540936979013,
                "rougeL_r": 0.3467605750591625,
                "rougeLsum_f": 0.20127878616337805,
                "rougeLsum_p": 0.15792540936979013,
                "rougeLsum_r": 0.3467605750591625
            },
            "bert_score": {
                "bertscore_precision": 0.8061001300811768,
                "bertscore_recall": 0.8419538736343384,
                "bertscore_f1": 0.8234115839004517
            },
            "meteor": 0.21212032478709097,
            "bleu": 0.034758945905171715,
            "distinct_ngram": {
                "distinct_1": 0.36408147277712494,
                "distinct_2": 0.6820249159580779,
                "distinct_3": 0.7735623003194888,
                "distinct_4": 0.7935067553942327
            },
            "word_entropy": 9.137783757476786,
            "semantic_similarity": 0.5317982311546803,
            "toxicity": {
                "toxic": 0.0022783192817587405
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.05202283722659722,
            "meteor": 0.2586628092089009,
            "rouge": {
                "rougeL_f": 0.2500611705571852
            },
            "bert_score": {
                "bertscore_precision": 0.592379093170166,
                "bertscore_recall": 0.6853244304656982,
                "bertscore_f1": 0.6334099173545837
            },
            "semantic_similarity": 0.531291221678257,
            "toxicity": {
                "toxic": 0.029045576199423523
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.58,
            "f1_score": 0.7272727272727273,
            "precision": 0.5957446808510638,
            "recall": 0.9333333333333333
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "llama2-7b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.34,
            "accuracy": 0.34,
            "f1_score": 0.3001449275362319,
            "precision": 0.3965367965367965,
            "recall": 0.34
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2142269377858024,
                "rouge1_p": 0.2768809539371684,
                "rouge1_r": 0.1943505387101138,
                "rouge2_f": 0.0671292704944741,
                "rouge2_p": 0.08487825100559,
                "rouge2_r": 0.06084399195548089,
                "rougeL_f": 0.1969015401333094,
                "rougeL_p": 0.2544660493972638,
                "rougeL_r": 0.17939002829976697
            },
            "bert_score": {
                "bertscore_precision": 0.7963831424713135,
                "bertscore_recall": 0.7952633500099182,
                "bertscore_f1": 0.7954913377761841
            },
            "distinct_ngram": {
                "distinct_1": 0.17480359147025815,
                "distinct_2": 0.3867387592487194,
                "distinct_3": 0.45640877598152424
            },
            "word_entropy": 8.016015980938764,
            "toxicity": {
                "toxic": 0.005686844593146816
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.22459221852115333,
                "rouge1_p": 0.1747198200855967,
                "rouge1_r": 0.36492718005923397,
                "rouge2_f": 0.06957941968614635,
                "rouge2_p": 0.04824865894020953,
                "rouge2_r": 0.14276203141507607,
                "rougeL_f": 0.20700361312165494,
                "rougeL_p": 0.1605963814088762,
                "rougeL_r": 0.33847457177376344,
                "rougeLsum_f": 0.20700361312165494,
                "rougeLsum_p": 0.1605963814088762,
                "rougeLsum_r": 0.33847457177376344
            },
            "bert_score": {
                "bertscore_precision": 0.8044140338897705,
                "bertscore_recall": 0.8388946652412415,
                "bertscore_f1": 0.8211774230003357
            },
            "meteor": 0.2091339422012403,
            "bleu": 0.035279456189579686,
            "distinct_ngram": {
                "distinct_1": 0.36508273228680527,
                "distinct_2": 0.6840300107181136,
                "distinct_3": 0.7816291161178509,
                "distinct_4": 0.8105977665863806
            },
            "word_entropy": 9.077406575931672,
            "semantic_similarity": 0.5507720416039228,
            "toxicity": {
                "toxic": 0.013780669302213937
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.1339538078970193,
            "meteor": 0.3065956505145932,
            "rouge": {
                "rougeL_f": 0.33545900395390504
            },
            "bert_score": {
                "bertscore_precision": 0.7748383283615112,
                "bertscore_recall": 0.8016892075538635,
                "bertscore_f1": 0.7870602607727051
            },
            "semantic_similarity": 0.7418132495880126,
            "toxicity": {
                "toxic": 0.003410458113066852
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "llama3-8b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.26,
            "accuracy": 0.26,
            "f1_score": 0.2058722976370035,
            "precision": 0.37666666666666665,
            "recall": 0.26
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.3463624670033557,
                "rouge1_p": 0.4313593540354754,
                "rouge1_r": 0.3251806628129426,
                "rouge2_f": 0.1318792163688263,
                "rouge2_p": 0.1741848414214286,
                "rouge2_r": 0.12981274730117512,
                "rougeL_f": 0.3121058662126471,
                "rougeL_p": 0.3828409389200964,
                "rougeL_r": 0.2937063951147287
            },
            "bert_score": {
                "bertscore_precision": 0.7389988899230957,
                "bertscore_recall": 0.7114666700363159,
                "bertscore_f1": 0.7246501445770264
            },
            "distinct_ngram": {
                "distinct_1": 0.21654875422946784,
                "distinct_2": 0.48659600997506236,
                "distinct_3": 0.5930489731437599
            },
            "word_entropy": 8.284307561015252,
            "toxicity": {
                "toxic": 0.002392488119658083
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.21094067365289376,
                "rouge1_p": 0.16847242250799233,
                "rouge1_r": 0.34944108591266565,
                "rouge2_f": 0.064767025875799,
                "rouge2_p": 0.04546235482163479,
                "rouge2_r": 0.1368926202818165,
                "rougeL_f": 0.19675173752965733,
                "rougeL_p": 0.1562300742254482,
                "rougeL_r": 0.328734328936701,
                "rougeLsum_f": 0.19675173752965733,
                "rougeLsum_p": 0.1562300742254482,
                "rougeLsum_r": 0.328734328936701
            },
            "bert_score": {
                "bertscore_precision": 0.8211895823478699,
                "bertscore_recall": 0.8564806580543518,
                "bertscore_f1": 0.8382613658905029
            },
            "meteor": 0.20670972553506647,
            "bleu": 0.033007635928418995,
            "distinct_ngram": {
                "distinct_1": 0.3374164810690423,
                "distinct_2": 0.6315099288122893,
                "distinct_3": 0.7186081694402421,
                "distinct_4": 0.7361588392516227
            },
            "word_entropy": 9.05348887887659,
            "semantic_similarity": 0.5516497104242444,
            "toxicity": {
                "toxic": 0.0028006199887022376
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.07668806911718518,
            "meteor": 0.2947626267085073,
            "rouge": {
                "rougeL_f": 0.3060470613135573
            },
            "bert_score": {
                "bertscore_precision": 0.68413245677948,
                "bertscore_recall": 0.7479808926582336,
                "bertscore_f1": 0.7120704650878906
            },
            "semantic_similarity": 0.638110869973898,
            "toxicity": {
                "toxic": 0.015429323447169735
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.4230769230769231,
            "precision": 0.5,
            "recall": 0.36666666666666664
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "falcon - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.24,
            "accuracy": 0.24,
            "f1_score": 0.0929032258064516,
            "precision": 0.0576,
            "recall": 0.24
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.29558139203111033,
                "rouge1_p": 0.4242762607671465,
                "rouge1_r": 0.25355633770985375,
                "rouge2_f": 0.10032467648444333,
                "rouge2_p": 0.14239215723069706,
                "rouge2_r": 0.09031783362971002,
                "rougeL_f": 0.27181157067900175,
                "rougeL_p": 0.3861552497826317,
                "rougeL_r": 0.23491628264238387
            },
            "bert_score": {
                "bertscore_precision": 0.8492820858955383,
                "bertscore_recall": 0.8148727416992188,
                "bertscore_f1": 0.831525444984436
            },
            "distinct_ngram": {
                "distinct_1": 0.1717827626918536,
                "distinct_2": 0.402636309167166,
                "distinct_3": 0.5059288537549407
            },
            "word_entropy": 7.844810360031424,
            "toxicity": {
                "toxic": 0.0022210937656927856
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.22334830717768867,
                "rouge1_p": 0.19988827470745116,
                "rouge1_r": 0.2992404968292302,
                "rouge2_f": 0.06543256390533038,
                "rouge2_p": 0.05220456716943173,
                "rouge2_r": 0.10937388629991703,
                "rougeL_f": 0.21150239827956668,
                "rougeL_p": 0.1885016308675204,
                "rougeL_r": 0.28412261844240383,
                "rougeLsum_f": 0.21150239827956668,
                "rougeLsum_p": 0.1885016308675204,
                "rougeLsum_r": 0.28412261844240383
            },
            "bert_score": {
                "bertscore_precision": 0.7989838123321533,
                "bertscore_recall": 0.8288270831108093,
                "bertscore_f1": 0.8134940266609192
            },
            "meteor": 0.17342143619300487,
            "bleu": 0.023502197730830236,
            "distinct_ngram": {
                "distinct_1": 0.24776119402985075,
                "distinct_2": 0.475993221615515,
                "distinct_3": 0.5473204104903079,
                "distinct_4": 0.5727987723000192
            },
            "word_entropy": 8.53416157477724,
            "semantic_similarity": 0.4915558641403914,
            "toxicity": {
                "toxic": 0.00867562529630959
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.046614332993276365,
            "meteor": 0.23907134040520084,
            "rouge": {
                "rougeL_f": 0.2449543445636978
            },
            "bert_score": {
                "bertscore_precision": 0.5554315447807312,
                "bertscore_recall": 0.7194968461990356,
                "bertscore_f1": 0.6247907280921936
            },
            "semantic_similarity": 0.40730362515896557,
            "toxicity": {
                "toxic": 0.0037966397975105794
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "falcon - 8quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.24,
            "accuracy": 0.24,
            "f1_score": 0.0929032258064516,
            "precision": 0.0576,
            "recall": 0.24
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.2896645057060388,
                "rouge1_p": 0.42487948601864245,
                "rouge1_r": 0.23718245053628015,
                "rouge2_f": 0.10179139037431735,
                "rouge2_p": 0.15140543420855615,
                "rouge2_r": 0.0832152508389598,
                "rougeL_f": 0.2692000658329764,
                "rougeL_p": 0.39247693240620307,
                "rougeL_r": 0.22126568894952395
            },
            "bert_score": {
                "bertscore_precision": 0.8427650928497314,
                "bertscore_recall": 0.8112103939056396,
                "bertscore_f1": 0.826545238494873
            },
            "distinct_ngram": {
                "distinct_1": 0.15382521099918323,
                "distinct_2": 0.3610267733922164,
                "distinct_3": 0.45256087321578503
            },
            "word_entropy": 7.86455772341483,
            "toxicity": {
                "toxic": 0.004116963855922222
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.2253699206715261,
                "rouge1_p": 0.1893932164761221,
                "rouge1_r": 0.3319389659359078,
                "rouge2_f": 0.06738181931941303,
                "rouge2_p": 0.0497080748386236,
                "rouge2_r": 0.1300641297549679,
                "rougeL_f": 0.21035423621208443,
                "rougeL_p": 0.17656805452885907,
                "rougeL_r": 0.31076811657351683,
                "rougeLsum_f": 0.21035423621208443,
                "rougeLsum_p": 0.17656805452885907,
                "rougeLsum_r": 0.31076811657351683
            },
            "bert_score": {
                "bertscore_precision": 0.7995340824127197,
                "bertscore_recall": 0.8335087299346924,
                "bertscore_f1": 0.815997838973999
            },
            "meteor": 0.18009062514386856,
            "bleu": 0.024414517975808146,
            "distinct_ngram": {
                "distinct_1": 0.2676822633297062,
                "distinct_2": 0.5171088746569076,
                "distinct_3": 0.6046898079763663,
                "distinct_4": 0.6349916154276132
            },
            "word_entropy": 8.585257516574215,
            "semantic_similarity": 0.49394595727324486,
            "toxicity": {
                "toxic": 0.004390336470678448
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.04905330789039819,
            "meteor": 0.2476805487954254,
            "rouge": {
                "rougeL_f": 0.24662064863263908
            },
            "bert_score": {
                "bertscore_precision": 0.5527916550636292,
                "bertscore_recall": 0.7215385437011719,
                "bertscore_f1": 0.623612642288208
            },
            "semantic_similarity": 0.37252742688171564,
            "toxicity": {
                "toxic": 0.02573598235845566
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "Ministral-8B - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.64,
            "accuracy": 0.64,
            "f1_score": 0.6364358786993058,
            "precision": 0.6724329004329005,
            "recall": 0.64
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.1,
            "rouge": {
                "rouge1_f": 0.3664114128327018,
                "rouge1_p": 0.32338180570642755,
                "rouge1_r": 0.451242337255432,
                "rouge2_f": 0.12925195934760392,
                "rouge2_p": 0.1085708034474436,
                "rouge2_r": 0.1743815424477025,
                "rougeL_f": 0.32439584801670973,
                "rougeL_p": 0.2863913083622696,
                "rougeL_r": 0.39995279952414814
            },
            "bert_score": {
                "bertscore_precision": 0.8462582230567932,
                "bertscore_recall": 0.8601343035697937,
                "bertscore_f1": 0.8530368208885193
            },
            "distinct_ngram": {
                "distinct_1": 0.22894215966571366,
                "distinct_2": 0.6259728708027574,
                "distinct_3": 0.8162806386327861
            },
            "word_entropy": 8.441706434469912,
            "toxicity": {
                "toxic": 0.0013857645587995648
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.24042937279053184,
                "rouge1_p": 0.17948146016200756,
                "rouge1_r": 0.3915117558472519,
                "rouge2_f": 0.06703389997358351,
                "rouge2_p": 0.04752921243160228,
                "rouge2_r": 0.12685360369873222,
                "rougeL_f": 0.21872657793898617,
                "rougeL_p": 0.16312255816460508,
                "rougeL_r": 0.35723257577511697,
                "rougeLsum_f": 0.21872657793898617,
                "rougeLsum_p": 0.16312255816460508,
                "rougeLsum_r": 0.35723257577511697
            },
            "bert_score": {
                "bertscore_precision": 0.8447073101997375,
                "bertscore_recall": 0.8706204295158386,
                "bertscore_f1": 0.8573780655860901
            },
            "meteor": 0.2361281035115487,
            "bleu": 0.0279381534967184,
            "distinct_ngram": {
                "distinct_1": 0.3992419456727732,
                "distinct_2": 0.7637795275590551,
                "distinct_3": 0.8694342869434287,
                "distinct_4": 0.8906283974777125
            },
            "word_entropy": 9.193928029617997,
            "semantic_similarity": 0.6585602331161499,
            "toxicity": {
                "toxic": 0.0009987953410018236
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.07631195605550427,
            "meteor": 0.28309927088398296,
            "rouge": {
                "rougeL_f": 0.2363131266057506
            },
            "bert_score": {
                "bertscore_precision": 0.6234296560287476,
                "bertscore_recall": 0.7310812473297119,
                "bertscore_f1": 0.6724721789360046
            },
            "semantic_similarity": 0.5825951987504959,
            "toxicity": {
                "toxic": 0.0152270498406142
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.64,
            "f1_score": 0.7692307692307693,
            "precision": 0.625,
            "recall": 1.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "Ministral-8B - 8quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.68,
            "accuracy": 0.68,
            "f1_score": 0.6798872180451129,
            "precision": 0.7186666666666666,
            "recall": 0.68
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.2,
            "rouge": {
                "rouge1_f": 0.35073188655138,
                "rouge1_p": 0.30699357861680016,
                "rouge1_r": 0.44197970483198,
                "rouge2_f": 0.1252780623169934,
                "rouge2_p": 0.1057289074649272,
                "rouge2_r": 0.17212452602635167,
                "rougeL_f": 0.3116777536979849,
                "rougeL_p": 0.27252905477072387,
                "rougeL_r": 0.39382896713483767
            },
            "bert_score": {
                "bertscore_precision": 0.8440098762512207,
                "bertscore_recall": 0.8580202460289001,
                "bertscore_f1": 0.8508360385894775
            },
            "distinct_ngram": {
                "distinct_1": 0.23735235123690662,
                "distinct_2": 0.6463826910074375,
                "distinct_3": 0.8320036471392751
            },
            "word_entropy": 8.518660219598242,
            "toxicity": {
                "toxic": 0.0015509510540869086
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.2404851910566867,
                "rouge1_p": 0.1885206173038173,
                "rouge1_r": 0.37130401572991767,
                "rouge2_f": 0.06679595325379281,
                "rouge2_p": 0.048275649172506346,
                "rouge2_r": 0.11972497813555648,
                "rougeL_f": 0.22610255041421465,
                "rougeL_p": 0.17745286000358879,
                "rougeL_r": 0.34929178415748985,
                "rougeLsum_f": 0.22610255041421465,
                "rougeLsum_p": 0.17745286000358879,
                "rougeLsum_r": 0.34929178415748985
            },
            "bert_score": {
                "bertscore_precision": 0.8423815369606018,
                "bertscore_recall": 0.8655556440353394,
                "bertscore_f1": 0.8536989092826843
            },
            "meteor": 0.22232495612706593,
            "bleu": 0.026001114404516818,
            "distinct_ngram": {
                "distinct_1": 0.39335548172757473,
                "distinct_2": 0.7417693169092945,
                "distinct_3": 0.8407701019252548,
                "distinct_4": 0.8620847651775487
            },
            "word_entropy": 9.147604593258185,
            "semantic_similarity": 0.6234662476181984,
            "toxicity": {
                "toxic": 0.003586107569281012
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.08119189848321809,
            "meteor": 0.27967341070884755,
            "rouge": {
                "rougeL_f": 0.24219141366466201
            },
            "bert_score": {
                "bertscore_precision": 0.6327611207962036,
                "bertscore_recall": 0.7367870807647705,
                "bertscore_f1": 0.6800834536552429
            },
            "semantic_similarity": 0.5752697610855102,
            "toxicity": {
                "toxic": 0.0032070281717460603
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.66,
            "f1_score": 0.7733333333333333,
            "precision": 0.6444444444444445,
            "recall": 0.9666666666666667
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "gemma-7b-it - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.5,
            "accuracy": 0.5,
            "f1_score": 0.49545730994152054,
            "precision": 0.5783394383394383,
            "recall": 0.5
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.3239375596854713,
                "rouge1_p": 0.3048523884412444,
                "rouge1_r": 0.3841923716028371,
                "rouge2_f": 0.10586254534400912,
                "rouge2_p": 0.09389646711799632,
                "rouge2_r": 0.13625798835846464,
                "rougeL_f": 0.2883749180544752,
                "rougeL_p": 0.2708622658827598,
                "rougeL_r": 0.3438719379719241
            },
            "bert_score": {
                "bertscore_precision": 0.8437180519104004,
                "bertscore_recall": 0.8461050987243652,
                "bertscore_f1": 0.8448162078857422
            },
            "distinct_ngram": {
                "distinct_1": 0.21231979030144169,
                "distinct_2": 0.5472614840989399,
                "distinct_3": 0.74497543546226
            },
            "word_entropy": 8.066178821306064,
            "toxicity": {
                "toxic": 0.0009421078918967396
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.2508147266105814,
                "rouge1_p": 0.17617673668609385,
                "rouge1_r": 0.4605110067807419,
                "rouge2_f": 0.08247106786703927,
                "rouge2_p": 0.05464492996487007,
                "rouge2_r": 0.18275228238657337,
                "rougeL_f": 0.23419632420154735,
                "rougeL_p": 0.16435107311353825,
                "rougeL_r": 0.43114167408790677,
                "rougeLsum_f": 0.23419632420154735,
                "rougeLsum_p": 0.16435107311353825,
                "rougeLsum_r": 0.43114167408790677
            },
            "bert_score": {
                "bertscore_precision": 0.8452356457710266,
                "bertscore_recall": 0.8739455938339233,
                "bertscore_f1": 0.8592390418052673
            },
            "meteor": 0.26884914253325154,
            "bleu": 0.036559917486466316,
            "distinct_ngram": {
                "distinct_1": 0.4200037957866768,
                "distinct_2": 0.8287028166315387,
                "distinct_3": 0.9504739794931322,
                "distinct_4": 0.9761672201601875
            },
            "word_entropy": 9.398422349045283,
            "semantic_similarity": 0.6878555911779404,
            "toxicity": {
                "toxic": 0.0018595833994913846
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.0884094101021456,
            "meteor": 0.27236551316707347,
            "rouge": {
                "rougeL_f": 0.2723344474356901
            },
            "bert_score": {
                "bertscore_precision": 0.7036900520324707,
                "bertscore_recall": 0.7722080945968628,
                "bertscore_f1": 0.7343739867210388
            },
            "semantic_similarity": 0.5139842060208321,
            "toxicity": {
                "toxic": 0.0012454364681616426
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.98,
            "f1_score": 0.9787234042553191,
            "precision": 1.0,
            "recall": 0.9583333333333334
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.5,
            "f1_score": 0.4444444444444444,
            "precision": 0.6666666666666666,
            "recall": 0.3333333333333333
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "llama2-7b - 8 quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.26,
            "accuracy": 0.26,
            "f1_score": 0.249435736677116,
            "precision": 0.2735531135531135,
            "recall": 0.26
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.261944337329925,
                "rouge1_p": 0.32115954777385936,
                "rouge1_r": 0.2454602465995767,
                "rouge2_f": 0.0730759150050963,
                "rouge2_p": 0.08729879456396848,
                "rouge2_r": 0.07073984513991134,
                "rougeL_f": 0.23446619654730896,
                "rougeL_p": 0.28911857018496734,
                "rougeL_r": 0.2189557576712333
            },
            "bert_score": {
                "bertscore_precision": 0.8060466647148132,
                "bertscore_recall": 0.8057874441146851,
                "bertscore_f1": 0.8056144118309021
            },
            "distinct_ngram": {
                "distinct_1": 0.19737217937732077,
                "distinct_2": 0.4601564763836569,
                "distinct_3": 0.5460158776830344
            },
            "word_entropy": 8.14510064060342,
            "toxicity": {
                "toxic": 0.003849600162357092
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.22770134948652926,
                "rouge1_p": 0.17247585352986666,
                "rouge1_r": 0.3846236967273808,
                "rouge2_f": 0.07650470187737295,
                "rouge2_p": 0.05365610491147033,
                "rouge2_r": 0.15612062103772925,
                "rougeL_f": 0.21016644036006704,
                "rougeL_p": 0.1589675096767048,
                "rougeL_r": 0.35632755528809645,
                "rougeLsum_f": 0.21016644036006704,
                "rougeLsum_p": 0.1589675096767048,
                "rougeLsum_r": 0.35632755528809645
            },
            "bert_score": {
                "bertscore_precision": 0.8097114562988281,
                "bertscore_recall": 0.8417410254478455,
                "bertscore_f1": 0.8252524733543396
            },
            "meteor": 0.2229849139242157,
            "bleu": 0.037249825297009376,
            "distinct_ngram": {
                "distinct_1": 0.3850478967097043,
                "distinct_2": 0.7258573532505785,
                "distinct_3": 0.8233418367346939,
                "distinct_4": 0.8515574650912997
            },
            "word_entropy": 9.239624530710003,
            "semantic_similarity": 0.5593912676349282,
            "toxicity": {
                "toxic": 0.0033512913039885463
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.0629976725607395,
            "meteor": 0.2876204537188742,
            "rouge": {
                "rougeL_f": 0.28490736154341983
            },
            "bert_score": {
                "bertscore_precision": 0.6177744269371033,
                "bertscore_recall": 0.7199385166168213,
                "bertscore_f1": 0.6615930795669556
            },
            "semantic_similarity": 0.5702503499388695,
            "toxicity": {
                "toxic": 0.005771477797534317
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "gemma-2-2b-it - 8quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.52,
            "accuracy": 0.52,
            "f1_score": 0.530345596432553,
            "precision": 0.6012662337662338,
            "recall": 0.52
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.38441274803016684,
                "rouge1_p": 0.3729979348021889,
                "rouge1_r": 0.43088058294127224,
                "rouge2_f": 0.13840698998256573,
                "rouge2_p": 0.12402434485000435,
                "rouge2_r": 0.17412645009844918,
                "rougeL_f": 0.3525623824767048,
                "rougeL_p": 0.341530491171084,
                "rougeL_r": 0.39678511850963893
            },
            "bert_score": {
                "bertscore_precision": 0.8458652496337891,
                "bertscore_recall": 0.8613789081573486,
                "bertscore_f1": 0.8535025119781494
            },
            "distinct_ngram": {
                "distinct_1": 0.24103225806451614,
                "distinct_2": 0.6376470588235295,
                "distinct_3": 0.8225165562913908
            },
            "word_entropy": 8.192090655362614,
            "toxicity": {
                "toxic": 0.002014050615252927
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.23375779569564867,
                "rouge1_p": 0.17829883488136328,
                "rouge1_r": 0.3859444372868142,
                "rouge2_f": 0.061816136411917,
                "rouge2_p": 0.042972776768316144,
                "rouge2_r": 0.12423080665719252,
                "rougeL_f": 0.21607972805437975,
                "rougeL_p": 0.16437136493092286,
                "rougeL_r": 0.35864698779240384,
                "rougeLsum_f": 0.21607972805437975,
                "rougeLsum_p": 0.16437136493092286,
                "rougeLsum_r": 0.35864698779240384
            },
            "bert_score": {
                "bertscore_precision": 0.8420835733413696,
                "bertscore_recall": 0.8665117025375366,
                "bertscore_f1": 0.8539962768554688
            },
            "meteor": 0.2330013848160861,
            "bleu": 0.025997459237469513,
            "distinct_ngram": {
                "distinct_1": 0.39557226399331663,
                "distinct_2": 0.7579147319544112,
                "distinct_3": 0.8570819112627986,
                "distinct_4": 0.8788270806382061
            },
            "word_entropy": 9.189864329712734,
            "semantic_similarity": 0.6497980868816375,
            "toxicity": {
                "toxic": 0.001000322960317135
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.07083156080362135,
            "meteor": 0.21764988994600296,
            "rouge": {
                "rougeL_f": 0.23023943199470823
            },
            "bert_score": {
                "bertscore_precision": 0.5114831328392029,
                "bertscore_recall": 0.5916517972946167,
                "bertscore_f1": 0.5472151637077332
            },
            "semantic_similarity": 0.504041351750493,
            "toxicity": {
                "toxic": 0.0015185511577874421
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.9,
            "f1_score": 0.8888888888888888,
            "precision": 0.9523809523809523,
            "recall": 0.8333333333333334
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.42,
            "f1_score": 0.17142857142857143,
            "precision": 0.6,
            "recall": 0.1
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": -0.1331088427631512,
            "spearman_correlation": -0.10948180550747302
        }
    },
    "gemma-3-4b-it - 8quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.28,
            "accuracy": 0.28,
            "f1_score": 0.2612144612144612,
            "precision": 0.28426666666666667,
            "recall": 0.28
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.007150692414157507,
                "rouge1_p": 0.0054029657488834605,
                "rouge1_r": 0.01128897731040525,
                "rouge2_f": 0.0,
                "rouge2_p": 0.0,
                "rouge2_r": 0.0,
                "rougeL_f": 0.006199235781820184,
                "rougeL_p": 0.004645682491600204,
                "rougeL_r": 0.009962890989140532
            },
            "bert_score": {
                "bertscore_precision": 0.6554579138755798,
                "bertscore_recall": 0.7487167119979858,
                "bertscore_f1": 0.6989200711250305
            },
            "distinct_ngram": {
                "distinct_1": 0.9637370939309997,
                "distinct_2": 1.0,
                "distinct_3": 1.0
            },
            "word_entropy": 11.869027386314219,
            "toxicity": {
                "toxic": 0.007176816067658365
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.012795237905342831,
                "rouge1_p": 0.008809099386020525,
                "rouge1_r": 0.02495463015578064,
                "rouge2_f": 0.0,
                "rouge2_p": 0.0,
                "rouge2_r": 0.0,
                "rougeL_f": 0.012025722552031708,
                "rougeL_p": 0.008302689129610269,
                "rougeL_r": 0.02333125353240402,
                "rougeLsum_f": 0.012025722552031708,
                "rougeLsum_p": 0.008302689129610269,
                "rougeLsum_r": 0.02333125353240402
            },
            "bert_score": {
                "bertscore_precision": 0.643165647983551,
                "bertscore_recall": 0.7876733541488647,
                "bertscore_f1": 0.7080679535865784
            },
            "meteor": 0.015740700608874943,
            "bleu": 0.00011275756309390511,
            "distinct_ngram": {
                "distinct_1": 0.9477931904161412,
                "distinct_2": 1.0,
                "distinct_3": 1.0,
                "distinct_4": 1.0
            },
            "word_entropy": 11.812159424145538,
            "semantic_similarity": 0.03149496865458787,
            "toxicity": {
                "toxic": 0.01493151425383985
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 6.551533064872689e-05,
            "meteor": 0.002235351180488925,
            "rouge": {
                "rougeL_f": 0.0017407302899493666
            },
            "bert_score": {
                "bertscore_precision": 0.37211939692497253,
                "bertscore_recall": 0.5317731499671936,
                "bertscore_f1": 0.436075896024704
            },
            "semantic_similarity": 0.14248560052365064,
            "toxicity": {
                "toxic": 0.008151537892408668
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": -0.15815391949030805,
            "spearman_correlation": -0.11671222631627302
        }
    },
    "gemma-3-1b-it full prec": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.26,
            "accuracy": 0.26,
            "f1_score": 0.19054302422723474,
            "precision": 0.3904761904761904,
            "recall": 0.26
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.3438247364405619,
                "rouge1_p": 0.3289797144128812,
                "rouge1_r": 0.3948547259540747,
                "rouge2_f": 0.11356640673810998,
                "rouge2_p": 0.10090117251670247,
                "rouge2_r": 0.14513486038844825,
                "rougeL_f": 0.31751359346704144,
                "rougeL_p": 0.30290810976889115,
                "rougeL_r": 0.3668110335562477
            },
            "bert_score": {
                "bertscore_precision": 0.8326959013938904,
                "bertscore_recall": 0.8464388847351074,
                "bertscore_f1": 0.8393898010253906
            },
            "distinct_ngram": {
                "distinct_1": 0.22804597701149426,
                "distinct_2": 0.5988372093023255,
                "distinct_3": 0.7767058823529411
            },
            "word_entropy": 8.170091241436031,
            "toxicity": {
                "toxic": 0.0017443108558654785
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.2056431371219025,
                "rouge1_p": 0.14681499647259555,
                "rouge1_r": 0.3633370806556585,
                "rouge2_f": 0.048633244776831355,
                "rouge2_p": 0.032732294874414924,
                "rouge2_r": 0.10235558094143055,
                "rougeL_f": 0.18846909660640443,
                "rougeL_p": 0.1343925768839031,
                "rougeL_r": 0.33452042133584015,
                "rougeLsum_f": 0.18846909660640443,
                "rougeLsum_p": 0.1343925768839031,
                "rougeLsum_r": 0.33452042133584015
            },
            "bert_score": {
                "bertscore_precision": 0.8376826643943787,
                "bertscore_recall": 0.8665187358856201,
                "bertscore_f1": 0.8517755270004272
            },
            "meteor": 0.2048111843708735,
            "bleu": 0.0183433105785957,
            "distinct_ngram": {
                "distinct_1": 0.42383440514469456,
                "distinct_2": 0.8233861144945189,
                "distinct_3": 0.948318293683347,
                "distinct_4": 0.9759635308744302
            },
            "word_entropy": 9.307155553293088,
            "semantic_similarity": 0.6613674706220627,
            "toxicity": {
                "toxic": 0.0008276531309820711
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.0795218102954683,
            "meteor": 0.2875000138158963,
            "rouge": {
                "rougeL_f": 0.26224665704549693
            },
            "bert_score": {
                "bertscore_precision": 0.6703275442123413,
                "bertscore_recall": 0.7541943192481995,
                "bertscore_f1": 0.7091017961502075
            },
            "semantic_similarity": 0.5520293751358986,
            "toxicity": {
                "toxic": 0.0026539108878932895
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.6,
            "f1_score": 0.75,
            "precision": 0.6,
            "recall": 1.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "qwen - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.6,
            "accuracy": 0.6,
            "f1_score": 0.585287356321839,
            "precision": 0.6272527472527473,
            "recall": 0.6
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.35945940909627294,
                "rouge1_p": 0.31805316688944596,
                "rouge1_r": 0.449504208175539,
                "rouge2_f": 0.11839703286016068,
                "rouge2_p": 0.10354180674938078,
                "rouge2_r": 0.15300398767363993,
                "rougeL_f": 0.31381937015172706,
                "rougeL_p": 0.27715990417058156,
                "rougeL_r": 0.3930276383328801
            },
            "bert_score": {
                "bertscore_precision": 0.8095414042472839,
                "bertscore_recall": 0.8345210552215576,
                "bertscore_f1": 0.8216938972473145
            },
            "distinct_ngram": {
                "distinct_1": 0.3139684998709011,
                "distinct_2": 0.7590897201150929,
                "distinct_3": 0.8995494301616751
            },
            "word_entropy": 8.755890493137846,
            "toxicity": {
                "toxic": 0.0012268098921049387
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.22097988079001327,
                "rouge1_p": 0.16928574386222903,
                "rouge1_r": 0.3526158186159234,
                "rouge2_f": 0.05802142808450213,
                "rouge2_p": 0.040399431181122374,
                "rouge2_r": 0.1145215120633389,
                "rougeL_f": 0.2055318122222925,
                "rougeL_p": 0.15706345457208015,
                "rougeL_r": 0.3295724294143029,
                "rougeLsum_f": 0.2055318122222925,
                "rougeLsum_p": 0.15706345457208015,
                "rougeLsum_r": 0.3295724294143029
            },
            "bert_score": {
                "bertscore_precision": 0.8307552337646484,
                "bertscore_recall": 0.8607529401779175,
                "bertscore_f1": 0.8453671932220459
            },
            "meteor": 0.2054252026087881,
            "bleu": 0.023892080113389794,
            "distinct_ngram": {
                "distinct_1": 0.3321872015281757,
                "distinct_2": 0.6628736740597878,
                "distinct_3": 0.7647517039922104,
                "distinct_4": 0.7933136676499508
            },
            "word_entropy": 8.985048742898663,
            "semantic_similarity": 0.6034972804784775,
            "toxicity": {
                "toxic": 0.003064482529880479
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.08516030151580672,
            "meteor": 0.30344130162105276,
            "rouge": {
                "rougeL_f": 0.2895870576139287
            },
            "bert_score": {
                "bertscore_precision": 0.7222082018852234,
                "bertscore_recall": 0.7851768732070923,
                "bertscore_f1": 0.7514500617980957
            },
            "semantic_similarity": 0.6311991065740585,
            "toxicity": {
                "toxic": 0.0020806593389716
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.98,
            "f1_score": 0.9795918367346939,
            "precision": 0.96,
            "recall": 1.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.68,
            "f1_score": 0.7894736842105263,
            "precision": 0.6521739130434783,
            "recall": 1.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    },
    "gemma-2-9b - 4quant": {
        "MMLU (all Subset - Templated)": {
            "exact_match": 0.42,
            "accuracy": 0.42,
            "f1_score": 0.3774358974358974,
            "precision": 0.5920000000000001,
            "recall": 0.42
        },
        "GSM8K (Reasoning Text Analysis)": {
            "exact_match": 0.0,
            "rouge": {
                "rouge1_f": 0.36961908311929853,
                "rouge1_p": 0.4410586683409434,
                "rouge1_r": 0.36790028322867896,
                "rouge2_f": 0.14275975048666276,
                "rouge2_p": 0.18193166416431975,
                "rouge2_r": 0.1501654805053456,
                "rougeL_f": 0.3401399660708427,
                "rougeL_p": 0.4013646547250523,
                "rougeL_r": 0.3403310716176364
            },
            "bert_score": {
                "bertscore_precision": 0.8721150159835815,
                "bertscore_recall": 0.8335110545158386,
                "bertscore_f1": 0.8519150018692017
            },
            "distinct_ngram": {
                "distinct_1": 0.2305902878743821,
                "distinct_2": 0.5718501032753025,
                "distinct_3": 0.7032045522611561
            },
            "word_entropy": 8.322500140015087,
            "toxicity": {
                "toxic": 0.0028535051713697614
            }
        },
        "CNN/DailyMail Summarization": {
            "rouge": {
                "rouge1_f": 0.21209980655590627,
                "rouge1_p": 0.17957168808585805,
                "rouge1_r": 0.32147905763036005,
                "rouge2_f": 0.05713471143280227,
                "rouge2_p": 0.04029666233986533,
                "rouge2_r": 0.11209016603977041,
                "rougeL_f": 0.19799875580837883,
                "rougeL_p": 0.16736564783569727,
                "rougeL_r": 0.30223559106825715,
                "rougeLsum_f": 0.19799875580837883,
                "rougeLsum_p": 0.16736564783569727,
                "rougeLsum_r": 0.30223559106825715
            },
            "bert_score": {
                "bertscore_precision": 0.8350477814674377,
                "bertscore_recall": 0.856191098690033,
                "bertscore_f1": 0.8452722430229187
            },
            "meteor": 0.19033582900096582,
            "bleu": 0.03023157082809874,
            "distinct_ngram": {
                "distinct_1": 0.34460052677787534,
                "distinct_2": 0.6606746560142033,
                "distinct_3": 0.7538150807899462,
                "distinct_4": 0.7796187017703132
            },
            "word_entropy": 8.86887739088292,
            "semantic_similarity": 0.5542145369946957,
            "toxicity": {
                "toxic": 0.005715991255152515
            }
        },
        "OPUS-100 English-to-French": {
            "bleu": 0.05465795324251764,
            "meteor": 0.26603109046072,
            "rouge": {
                "rougeL_f": 0.27500012779117333
            },
            "bert_score": {
                "bertscore_precision": 0.6069121360778809,
                "bertscore_recall": 0.7227703332901001,
                "bertscore_f1": 0.6564710140228271
            },
            "semantic_similarity": 0.49315946877002714,
            "toxicity": {
                "toxic": 0.016617706652032212
            }
        },
        "GLUE SST-2 (Prompting - Improved)": {
            "accuracy": 0.52,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE MRPC (Prompting - Improved)": {
            "accuracy": 0.4,
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0
        },
        "GLUE STS-B (Prompting - Improved)": {
            "pearson_correlation": 0.0,
            "spearman_correlation": 0.0
        }
    }
}
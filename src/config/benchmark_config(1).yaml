advanced:
  distributed_training: false
  enable_multi_gpu: false
  use_tpu: false
evaluation:
  batch_size: 16
  log_interval: 100
general:
  experiment_name: Gemma_Benchmark_2025
  output_dir: ./benchmarks
  random_seed: 42
model_parameters:
  batch_size: 4
  max_input_length: 512
  max_output_length: 512
  temperature: 0.7
  top_k: 50
  top_p: 0.9
models:
- checkpoint: gemma-7b-checkpoint
  framework: huggingface
  name: gemma-7b
  offloading: true
  quantization: 4bit
  size: 7B
  variant: gemma
- checkpoint: gemma-13b-checkpoint
  framework: huggingface
  name: gemma-13b
  offloading: true
  quantization: 8bit
  size: 13B
  variant: gemma
reporting:
  enabled: true
  format: pdf
  generate_visuals:
    charts: true
    save_plots: true
    tables: true
  leaderboard_enabled: true
  output_dir: ./reports
tasks:
- datasets:
  - name: mmlu
    splits:
    - train
    - validation
    type: classification
  description: Massive Multitask Language Understanding
  evaluation_metrics:
  - accuracy
  - f1_score
  - perplexity
  name: MMLU
  type: classification
- datasets:
  - name: gsm8k
    splits:
    - train
    - validation
    type: generation
  description: Math dataset with problem-solving tasks
  evaluation_metrics:
  - accuracy
  - execution_time
  name: GSM8K
  type: generation

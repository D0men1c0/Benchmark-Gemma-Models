# ============================
# General Configuration
# ============================
general:
  experiment_name: "Gemma_Benchmark_2025"
  output_dir: "./benchmarks"
  random_seed: 42

# ============================
# Benchmark Tasks Configuration
# ============================
tasks:
  - name: "MMLU"
    type: "generation" # Task type
    description: "Massive Multitask Language Understanding"
    datasets:
      - name: "cais/mmlu"
        source_type: "hf_hub" # Assuming Hugging Face Hub
        config: "anatomy" # Specify config if needed, e.g., 'all' for MMLU
        split: "validation" # Or "test", "train" etc.
    evaluation_metrics:
      - name: exact_match
        options: { normalize: true, ignore_case: true, ignore_punct: true }

  - name: "GSM8K"
    type: "generation" # Task type
    description: "Math dataset with problem-solving tasks"
    datasets:
      - name: "gsm8k"
        source_type: "hf_hub" # Assuming Hugging Face Hub
        config: "main" # Specify config if needed, e.g., 'main' for GSM8K
        split: "test" # Or "train"
    evaluation_metrics:
      - name: accuracy
      # - name: execution_time # Note: Requires implementation and registration

# ============================
# Models Configuration
# ============================
models:
  - name: "gemma-7b" # User-defined name for results reporting
    variant: "gemma"
    size: "7B"
    framework: "huggingface"
    checkpoint: "google/gemma-7b" # Actual HF model ID or path
    quantization: "4bit"
    offloading: true

  - name: "gemma-13b" # User-defined name
    variant: "gemma"
    size: "13B"
    framework: "huggingface"
    checkpoint: "google/gemma-13b" # Actual HF model ID or path
    quantization: "8bit"
    offloading: true

  - name: "llama-2-7b" # User-defined name
    variant: "llama"
    size: "7B"
    framework: "huggingface"
    checkpoint: "meta-llama/Llama-2-7b-hf" # Example actual HF ID
    quantization: "4bit"
    offloading: true

  - name: "mistral-7b" # User-defined name
    variant: "mistral"
    size: "7B"
    framework: "huggingface"
    checkpoint: "mistralai/Mistral-7B-v0.1" # Example actual HF ID
    quantization: "4bit"
    offloading: true

# ============================
# Model Parameters Configuration
# ============================
model_parameters:
  # Parameters for model loading/inference (e.g., generation config)
  # batch_size: 4 # Suggest moving batch_size to 'advanced' section
  max_input_length: 512 # Note: Actual truncation might depend on TaskHandler logic
  max_output_length: 512 # Note: Actual generation length often controlled by TaskHandler args
  #temperature: 0.7
  #top_p: 0.9
  #top_k: 50
  # Consider moving TaskHandler params like max_new_tokens, num_beams here if reorganized

# ============================
# Evaluation Configuration
# ============================
evaluation:
  # Parameters specific to the evaluation process itself (if any)
  # batch_size: 16 # Only if evaluation needs a *different* batch size than inference
  log_interval: 100

# ============================
# Results and Reporting
# ============================
reporting:
  enabled: true
  format: "pdf" # Or "json", "csv", etc.
  leaderboard_enabled: true
  generate_visuals:
    charts: true
    tables: true
    save_plots: true
  output_dir: "./reports"

# ============================
# Advanced Settings (Runtime/Environment)
# ============================
advanced:
  enable_multi_gpu: false
  use_tpu: false
  distributed_training: false
  batch_size: 8 # Main batch size for DataLoader/inference
  # TaskHandler/Tokenizer/Generate parameters could be moved from ModelParamsConfig or defined here
  truncation: true
  padding: true
  generate_max_length: 512 # Max length potentially used by tokenizer/generator
  skip_special_tokens: true
  max_new_tokens: 50 # Max tokens to generate *new*
  num_beams: 1
  do_sample: false
  use_cache: false # Can set to true for faster generation (uses more memory)
  clean_up_tokenization_spaces: true
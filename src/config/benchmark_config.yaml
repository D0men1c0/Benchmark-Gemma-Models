# ============================
# General Configuration
# ============================
general:
  experiment_name: "Gemma_Benchmark_Comprehensive_2025"
  output_dir: "./benchmarks_output" # All benchmark outputs will be saved here
  random_seed: 42

# ============================
# Benchmark Tasks Configuration
# ============================
tasks:
  # --- Task 1: MMLU (Multiple Choice QA) ---
  - name: "MMLU (all Subset - Templated)"
    type: "multiple_choice_qa"
    description: "MMLU (all subjects) via templated letter generation."
    datasets:
      - name: "cais/mmlu"
        source_type: "hf_hub"
        config: "all" # Using all MMLU subjects
        split: "validation"
        max_samples: 10 # Limit to 10 samples for testing
    handler_options:
      prompt_builder_type: "mmlu" # Uses MMLUPromptBuilder
      prompt_template: |
        The following is a multiple-choice question about {subject}.
        Please choose the letter of the correct answer.
        Question: {question}
        Choices:
        {choices_formatted_str}
        Answer:
      default_subject: "the following topic"
    evaluation_metrics:
      - name: exact_match
        options: { normalize: true, ignore_case: true, ignore_punct: true }
      - name: accuracy
        options: {}
      - name: f1_score
        options: { average: "weighted" }
      - name: precision
        options: { average: "weighted" }
      - name: recall
        options: { average: "weighted" }
      - name: rouge
        options:
          metrics: ['rouge1', 'rouge2', 'rougeL']
          stats: ['f']                         
      - name: bleu
        options: {}
      - name: meteor
        options: {} 
      - name: bert_score
        options: { lang: "en" }

  # --- Task 2: GSM8K (Reasoning Text Analysis) ---
  - name: "GSM8K (Reasoning Text Analysis)"
    type: "math_reasoning_generation"
    description: "Grade school math problems. Evaluates the textual properties of the generated reasoning steps and the final answer."
    datasets:
      - name: "gsm8k"
        source_type: "hf_hub"
        config: "main"
        split: "test"
        max_samples: 10
    handler_options:
      prompt_builder_type: "default"
      prompt_template: "Solve the following math problem step-by-step. Your final answer should be a number, formatted as ####<number>.\n\nProblem: {input_text}\n\nSolution:"
      # For evaluating the final numerical answer, you need a postprocessor that extracts it.
      # 'gsm8k' postprocessor_key is designed for this.
      postprocessor_key: "gsm8k" # IMPORTANT: This extracts the final number for exact_match
    evaluation_metrics:
      - name: exact_match # Evaluates the correctness of the *extracted numerical answer*
        options: {}      # Normalization options might be useful depending on how numbers are formatted
      # The following metrics evaluate the properties of the *full generated reasoning text*:
      - name: rouge # Compares textual similarity of the reasoning steps
        options:
          metrics: ['rouge1', 'rouge2', 'rougeL']
          stats: ['f', 'p', 'r'] # You can choose f, p, r or combinations
      - name: bert_score # Compares semantic similarity of the reasoning steps
        options: { lang: "en" }
      - name: distinct_ngram # Calculates lexical diversity in the reasoning text
        options: { ngrams: [1, 2, 3] }
      - name: word_entropy # Another measure of text diversity/predictability
        options: {}
      # Perplexity could also be interesting for the generated reasoning steps
      # - name: perplexity
      #   options: {} # Requires model to output log_probs or for it to be calculated.

  # --- Task 3: Summarization (CNN/DailyMail) ---
  - name: "CNN/DailyMail Summarization"
    type: "summarization"
    description: "Abstractive summarization of news articles."
    datasets:
      - name: "cnn_dailymail"
        source_type: "hf_hub"
        config: "3.0.0"
        split: "test"
        max_samples: 10
    handler_options:
      prompt_builder_type: "default"
      prompt_template: "Summarize the following article in a few sentences:\n\nArticle: {article_text}\n\nSummary:"
      postprocessor_key: "summarization" # Uses default post-processing for summaries
    evaluation_metrics:
      - name: rouge
        options:
          metrics: ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
          stats: ['f', 'p', 'r'] # Common to report all for summarization
      - name: bert_score
        options: { lang: "en" }
      - name: meteor # Often used for summarization alongside ROUGE
        options: {}
      - name: bleu # Less common for summarization than ROUGE/METEOR but can be included
        options: {}
      - name: distinct_ngram # Lexical diversity of the summary
        options: { ngrams: [1, 2, 3, 4] } # Can check up to 4-grams
      - name: word_entropy # Entropy of the summary
        options: {}
      - name: semantic_similarity 
        options: { model: 'all-MiniLM-L6-v2', metric_type: "cosine" } # metric_type was a previous suggestion

  # --- Task 4: Translation (OPUS-100 English to French) ---
  - name: "OPUS-100 English-to-French"
    type: "translation"
    description: "Translate sentences from English to French using OPUS-100."
    datasets:
      - name: "opus100"
        source_type: "hf_hub"
        config: "en-fr" 
        split: "validation"
        max_samples: 10
    handler_options:
      prompt_builder_type: "translation" 
      prompt_template: "Translate the following {source_lang_name} text into {target_lang_name}: {input_text}"
      postprocessor_key: "translation" # Uses default post-processing for translations
    evaluation_metrics:
      - name: bleu
        options: {} # Default BLEU-4 with smoothing method1
                     # Can add: { weights: [0.25,0.25,0.25,0.25], smoothing: "method7" } for example
      - name: meteor
        options: {} # Default METEOR
                     # Can add: { alpha: 0.9, beta: 3.0, gamma: 0.5 }
      - name: rouge # ROUGE can also be used for translation, though less primary than BLEU/METEOR
        options:
          metrics: ['rougeL']
          stats: ['f']
      - name: bert_score # Semantic similarity
        options: { lang: "fr" } # Target language is French
      # Perplexity of the translated text
      # - name: perplexity
      #   options: {}
      # Semantic Similarity can also be used here
      - name: semantic_similarity
        options: { model: 'paraphrase-multilingual-MiniLM-L12-v2', metric_type: "cosine" } # A multilingual model

# ============================
# Models Configuration
# ============================
models:
  - name: "gemma-7b" # Using instruction-tuned version
    variant: "gemma"
    size: "7B"
    framework: "huggingface"
    checkpoint: "google/gemma-7b"
    quantization: "4bit" # Or "8bit" or null (None in Python)
    offloading: true
    torch_dtype: "bfloat16" # Use bfloat16 for better performance on GPUs with Tensor Cores

  - name: "gemma-2b"
    variant: "gemma"
    size: "2B"
    framework: "huggingface"
    checkpoint: "google/gemma-2b"
    quantization: "4bit"
    offloading: true
    torch_dtype: "bfloat16"

  - name: "gpt2" # Added GPT-2 base model
    variant: "gpt2"
    size: "124M" # Standard GPT-2 small
    framework: "huggingface"
    checkpoint: "gpt2"
    quantization: null # No quantization for gpt2, it's small
    offloading: false

  - name: "phi-2" # Alternative 2.7B parameter model (not Llama architecture)
    variant: "phi"
    size: "2.7B"
    framework: "huggingface"
    checkpoint: "microsoft/phi-2"
    quantization: "4bit" # or null if you have enough VRAM
    offloading: true

  - name: "llama2-7b"
    variant: "llama"
    size: "7B"
    framework: "huggingface"
    checkpoint: "meta-llama/Llama-2-7b-hf"
    quantization: "4bit"
    offloading: true
    torch_dtype: "bfloat16"

  - name: "llama3-8b"
    variant: "llama3"
    size: "8B"
    framework: "huggingface"
    checkpoint: "meta-llama/Meta-Llama-3-8B"
    quantization: "4bit"
    offloading: true
    torch_dtype: "bfloat16"

  - name: "mistral-7b"
    variant: "mistral"
    size: "7B"
    framework: "huggingface"
    checkpoint: "mistralai/Mistral-7B-v0.1"
    quantization: "4bit"
    offloading: true
    torch_dtype: "bfloat16"

  - name: "llama3-8b 8quant"
    variant: "llama3"
    size: "8B"
    framework: "huggingface"
    checkpoint: "meta-llama/Meta-Llama-3-8B"
    quantization: "8bit"
    offloading: true
    torch_dtype: "float16"

  - name: "mistral-7b 8quant"
    variant: "mistral"
    size: "7B"
    framework: "huggingface"
    checkpoint: "mistralai/Mistral-7B-v0.1"
    quantization: "8bit"
    offloading: true
    torch_dtype: "float16"


# ============================
# Model Parameters Configuration (Global for model loading, not generation)
# ============================
model_parameters:
  max_input_length: 512 # Used by some model loaders, but tokenizer truncation in TaskHandler is key
  max_output_length: 512 # Generic, actual generation length is by max_new_tokens in advanced settings

# ============================
# Evaluation Configuration
# ============================
evaluation:
  log_interval: 5 # Log intermediate metrics every 5 batches

# ============================
# Results and Reporting
# ============================
reporting:
  enabled: true
  format: "json" # Or "pdf", "csv"
  leaderboard_enabled: false
  generate_visuals:
    charts: true
    tables: true
    save_plots: true
  output_dir: "./reports"

# ============================
# Advanced Settings (Global runtime/generation parameters for TaskHandlers)
# ============================
advanced:
  enable_multi_gpu: false
  use_tpu: false
  distributed_training: false
  batch_size: 10      # Main batch_size for DataLoader/inference
  truncation: true    # For tokenizer
  padding: true       # For tokenizer ("longest" or a max_length strategy)
  generate_max_length: 512 # Max length for tokenizer input processing in TaskHandler's _generate_text
  skip_special_tokens: true # For tokenizer.decode
  max_new_tokens: 150  # Max *new* tokens for model.generate(). Increased from 50.
                      # Consider increasing this further for GSM8K (e.g., 256-512) or Summarization,
                      # or allow override in task_cfg.handler_options.
  num_beams: 1
  do_sample: false
  use_cache: true     # Setting to true is generally recommended for faster generation, uses more VRAM.
  clean_up_tokenization_spaces: true
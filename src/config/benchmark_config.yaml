# ============================
# General Configuration
# ============================
general:
  experiment_name: "Gemma_Benchmark_2025"
  output_dir: "./benchmarks"
  random_seed: 42

# ============================
# Benchmark Tasks Configuration
# ============================
tasks:
  # --- Task 1: MMLU (as Letter Generation) ---
  - name: "MMLU (Generation - Anatomy Subset)"
    type: "multiple_choice_qa"  # Use the specific type for the TaskHandlerFactory
    description: "MMLU (Anatomy subset) evaluated via letter generation."
    datasets:
      - name: "cais/mmlu"
        source_type: "hf_hub"
        config: "anatomy"      # USE ONLY ONE SUBSET (e.g., anatomy) for testing! 'all' is too long.
        split: "validation"
    evaluation_metrics:
      - name: exact_match # Key metric for this task setup
        options: { normalize: true, ignore_case: true, ignore_punct: true } # Useful options

  # --- Task 2: GSM8K (Reasoning + Answer Generation) ---
  - name: "GSM8K"
    type: "math_reasoning_generation" # Use the specific type for the TaskHandlerFactory
    description: "Grade school math problems requiring reasoning steps."
    datasets:
      - name: "gsm8k"
        source_type: "hf_hub"
        config: "main"
        split: "test"
    evaluation_metrics:
      - name: exact_match
        options: { normalize: true, ignore_case: true, ignore_punct: true }
      - name: distinct_ngram
        options: { ngrams: [1, 2] }

  # --- Task 3: Summarization (To test ROUGE, BERTScore etc.) ---
  - name: "CNN/DailyMail Summarization (Sample)"
    type: "summarization" # Use the specific type for the TaskHandlerFactory
    description: "Abstractive summarization benchmark on news articles (small sample)."
    datasets:
      - name: "cnn_dailymail"       
        source_type: "hf_hub"
        config: "3.0.0"           
        split: "test"       
    evaluation_metrics:
      - name: rouge
        options:
          metrics: ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
          stats: ['f']
      - name: bert_score
        options: { lang: "en" }
      - name: distinct_ngram
        options: { ngrams: [1, 2] }

# ============================
# Models Configuration
# ============================
models:
  - name: "gemma-7b" # User-defined name for results reporting
    variant: "gemma"
    size: "7B"
    framework: "huggingface"
    checkpoint: "google/gemma-7b" # Actual HF model ID or path
    quantization: "4bit"
    offloading: true

# ============================
# Model Parameters Configuration
# ============================
model_parameters:
  # Parameters for model loading/inference (e.g., generation config)
  # batch_size: 4 # Suggest moving batch_size to 'advanced' section
  max_input_length: 512 # Note: Actual truncation might depend on TaskHandler logic
  max_output_length: 512 # Note: Actual generation length often controlled by TaskHandler args
  #temperature: 0.7
  #top_p: 0.9
  #top_k: 50
  # Consider moving TaskHandler params like max_new_tokens, num_beams here if reorganized

# ============================
# Evaluation Configuration
# ============================
evaluation:
  # Parameters specific to the evaluation process itself (if any)
  # batch_size: 16 # Only if evaluation needs a *different* batch size than inference
  log_interval: 100

# ============================
# Results and Reporting
# ============================
reporting:
  enabled: true
  format: "pdf" # Or "json", "csv", etc.
  leaderboard_enabled: true
  generate_visuals:
    charts: true
    tables: true
    save_plots: true
  output_dir: "./reports"

# ============================
# Advanced Settings (Runtime/Environment)
# ============================
advanced:
  enable_multi_gpu: false
  use_tpu: false
  distributed_training: false
  batch_size: 8 # Main batch size for DataLoader/inference
  # TaskHandler/Tokenizer/Generate parameters could be moved from ModelParamsConfig or defined here
  truncation: true
  padding: true
  generate_max_length: 512 # Max length potentially used by tokenizer/generator
  skip_special_tokens: true
  max_new_tokens: 50 # Max tokens to generate *new*
  num_beams: 1
  do_sample: false
  use_cache: false # Can set to true for faster generation (uses more memory)
  clean_up_tokenization_spaces: true
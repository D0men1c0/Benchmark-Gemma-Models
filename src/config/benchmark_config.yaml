# ============================
# General Configuration
# ============================
general:
  experiment_name: "Gemma_Benchmark_Comprehensive_2025"
  output_dir: "./benchmarks_output" # All benchmark outputs will be saved here
  random_seed: 42

# ============================
# Benchmark Tasks Configuration
# ============================
tasks:
  # --- Task 1: MMLU (Multiple Choice QA) ---
  - name: "MMLU (all Subset - Templated)"
    type: "multiple_choice_qa"
    description: "MMLU (all subjects) via templated letter generation."
    datasets:
      - name: "cais/mmlu"
        source_type: "hf_hub"
        config: "all" # Using all MMLU subjects
        split: "validation"
        max_samples: 50 # Limit to 50 samples for testing
    handler_options:
      prompt_builder_type: "mmlu" # Uses MMLUPromptBuilder
      prompt_template: |
        Answer the following multiple-choice question about {subject} by providing only the letter of the correct option.
        Question: {question}
        Choices:
        {choices_formatted_str}
        Your answer (select one letter from A, B, C, D, etc.):
      default_subject: "the following topic"
    evaluation_metrics:
      # Metrics like ROUGE, BLEU, METEOR, BERTScore are generally not meaningful for single-letter classification outputs.
      # Kept classification-style metrics.
      - name: exact_match # Will compare the extracted letter (A,B,C,D) from model output with the label letter.
        options: { normalize: true, ignore_case: true, ignore_punct: false } # Punct should not matter for single letters
      - name: accuracy # Equivalent to exact_match if labels are single letters
        options: {}
      - name: f1_score # Treating A,B,C,D... as classes
        options: { average: "weighted" , zero_division: 0 }
      - name: precision
        options: { average: "weighted" , zero_division: 0 }
      - name: recall
        options: { average: "weighted" , zero_division: 0 }

  # --- Task 2: GSM8K (Reasoning Text Analysis) ---
  - name: "GSM8K (Reasoning Text Analysis)"
    type: "math_reasoning_generation"
    description: "Grade school math problems. Evaluates the textual properties of the generated reasoning steps and the final answer."
    datasets:
      - name: "gsm8k"
        source_type: "hf_hub"
        config: "main"
        split: "test"
        max_samples: 50
    handler_options:
      prompt_builder_type: "default"
      prompt_template: "Solve the following math problem. Show your reasoning step-by-step. Conclude with the final numerical answer formatted as ####<number>.\n\nProblem: {input_text}\n\nSolution:"
      postprocessor_key: "gsm8k" # IMPORTANT: This extracts the final number for exact_match
    evaluation_metrics:
      - name: exact_match # Evaluates the correctness of the *extracted numerical answer*
        options: {}
      - name: rouge # Compares textual similarity of the reasoning steps
        options:
          metrics: ['rouge1', 'rouge2', 'rougeL']
          stats: ['f', 'p', 'r']
      - name: bert_score # Compares semantic similarity of the reasoning steps
        options: { lang: "en" }
      - name: distinct_ngram # Calculates lexical diversity in the reasoning text
        options: { ngrams: [1, 2, 3] }
      - name: word_entropy # Another measure of text diversity/predictability
        options: {}
      - name: toxicity # Evaluate toxicity of the generated reasoning
        options: { target_label: "toxic" } # Options can include model, target_label, threshold

  # --- Task 3: Summarization (CNN/DailyMail) ---
  - name: "CNN/DailyMail Summarization"
    type: "summarization"
    description: "Abstractive summarization of news articles."
    datasets:
      - name: "cnn_dailymail"
        source_type: "hf_hub"
        config: "3.0.0"
        split: "test"
        max_samples: 50
    handler_options:
      prompt_builder_type: "default"
      prompt_template: "Provide a concise, abstractive summary of the following news article. Focus on the main points and key information.\n\nArticle: {article_text}\n\nSummary:"
      postprocessor_key: "summarization" # Uses default post-processing for summaries
    evaluation_metrics:
      - name: rouge
        options:
          metrics: ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
          stats: ['f', 'p', 'r']
      - name: bert_score
        options: { lang: "en" }
      - name: meteor
        options: {}
      - name: bleu
        options: {}
      - name: distinct_ngram
        options: { ngrams: [1, 2, 3, 4] }
      - name: word_entropy
        options: {}
      - name: semantic_similarity
        options: { model: 'all-MiniLM-L6-v2', metric: "cosine" } # 'metric' was 'metric_type'
      - name: toxicity # Evaluate toxicity of the generated summary
        options: { target_label: "toxic" }

  # --- Task 4: Translation (OPUS-100 English to French) ---
  - name: "OPUS-100 English-to-French"
    type: "translation"
    description: "Translate sentences from English to French using OPUS-100."
    datasets:
      - name: "opus100"
        source_type: "hf_hub"
        config: "en-fr"
        split: "validation"
        max_samples: 50
    handler_options:
      prompt_builder_type: "translation"
      prompt_template: "Translate the following {source_lang_name} text into {target_lang_name}.\n\n{source_lang_name} text: \"{input_text}\"\n\n{target_lang_name} text:"
      postprocessor_key: "translation" # Uses default post-processing for translations
    evaluation_metrics:
      - name: bleu
        options: {}
      - name: meteor
        options: {}
      - name: rouge
        options:
          metrics: ['rougeL']
          stats: ['f']
      - name: bert_score
        options: { lang: "fr" } # Target language is French
      - name: semantic_similarity
        options: { model: 'paraphrase-multilingual-MiniLM-L12-v2', metric: "cosine" } # 'metric' was 'metric_type'
      - name: toxicity # Evaluate toxicity of the generated translation
        options: { target_label: "toxic" }

  # --- Task 5: GLUE SST-2 (Sentiment Classification - Prompting) ---
  - name: "GLUE SST-2 (Prompting - Improved)"
    type: "classification"
    description: "GLUE SST-2 sentiment classification using improved prompting."
    datasets:
      - name: "glue"
        source_type: "hf_hub"
        config: "sst2"
        split: "validation"
        max_samples: 50
    handler: "glue_classification_prompting"
    handler_options:
      prompt_builder_type: "glue_sst2_prompt"
      prompt_template: | # Questo template verr√† usato dal GlueSST2PromptBuilder
        Example:
        Review: "this movie is absolutely fantastic, a masterpiece of cinema!"
        Sentiment (positive or negative): positive
        ###
        Review: "{input_text}"
        Sentiment (positive or negative):
      postprocessor_key: "glue_sst2_output"
      dataset_config_name: "sst2"
      max_new_tokens: 10

    evaluation_metrics:
      - name: "accuracy"
        options: {}
      - name: "f1_score"
        options: { average: "binary", pos_label: 1, zero_division: 0 }
      - name: "precision"
        options: { average: "binary", pos_label: 1, zero_division: 0 }
      - name: "recall"
        options: { average: "binary", pos_label: 1, zero_division: 0 }


  # --- Task 6: GLUE MRPC (Paraphrase Detection - Prompting) ---
  - name: "GLUE MRPC (Prompting - Improved)"
    type: "text_pair_classification"
    description: "GLUE MRPC paraphrase detection using improved prompting."
    datasets:
      - name: "glue"
        source_type: "hf_hub"
        config: "mrpc"
        split: "validation"
        max_samples: 50
    handler: "glue_text_pair_prompting"
    handler_options:
      prompt_builder_type: "glue_mrpc_prompt"
      prompt_template: |
        Example:
        Sentence 1: "The company chairman said he was confident that the share price would recover."
        Sentence 2: "The chairman expressed his belief that the company's stock would bounce back."
        Are these sentences paraphrases (yes or no)?
        Answer: yes
        ###
        Sentence 1: "{input_text_pair1}"
        Sentence 2: "{input_text_pair2}"
        Are these sentences paraphrases (yes or no)?
        Answer:
      postprocessor_key: "glue_mrpc_output"
      dataset_config_name: "mrpc"
      max_new_tokens: 10

    evaluation_metrics:
      - name: "accuracy"
        options: {}
      - name: "f1_score"
        options: { average: "binary", pos_label: 1, zero_division: 0 }
      - name: "precision"
        options: { average: "binary", pos_label: 1, zero_division: 0 }
      - name: "recall"
        options: { average: "binary", pos_label: 1, zero_division: 0 }

  # --- Task 7: GLUE STS-B (Semantic Similarity - Prompting) ---
  - name: "GLUE STS-B (Prompting - Improved)"
    type: "text_pair_classification"
    description: "GLUE STS-B semantic textual similarity using improved prompting."
    datasets:
      - name: "glue"
        source_type: "hf_hub"
        config: "stsb"
        split: "validation"
        max_samples: 50
    handler: "glue_text_pair_prompting"
    handler_options:
      prompt_builder_type: "glue_stsb_prompt"
      prompt_template: |
        Example:
        Sentence 1: "A plane is taking off."
        Sentence 2: "An air plane is taking off."
        Similarity Score (0.0-5.0): 5.0
        ###
        Sentence 1: "{input_text_pair1}"
        Sentence 2: "{input_text_pair2}"
        Similarity Score (0.0-5.0):
      postprocessor_key: "glue_stsb_output"
      dataset_config_name: "stsb"
      max_new_tokens: 10

    evaluation_metrics:
      - name: "pearson_correlation"
        options: {}
      - name: "spearman_correlation"
        options: {}

# ============================
# Models Configuration
# ============================
models:
  - name: "gemma-7b" # Using instruction-tuned version
    variant: "gemma"
    size: "7B"
    framework: "huggingface"
    checkpoint: "google/gemma-7b"
    quantization: "4bit" # Or "8bit" or null (None in Python)
    offloading: true
    torch_dtype: "bfloat16" # Use bfloat16 for better performance on GPUs with Tensor Cores

  - name: "gemma-2b"
    variant: "gemma"
    size: "2B"
    framework: "huggingface"
    checkpoint: "google/gemma-2b"
    quantization: "4bit"
    offloading: true
    torch_dtype: "bfloat16"

  - name: "gpt2" # Added GPT-2 base model
    variant: "gpt2"
    size: "124M" # Standard GPT-2 small
    framework: "huggingface"
    checkpoint: "gpt2"
    quantization: null # No quantization for gpt2, it's small
    offloading: false

  - name: "phi-2" # Alternative 2.7B parameter model (not Llama architecture)
    variant: "phi"
    size: "2.7B"
    framework: "huggingface"
    checkpoint: "microsoft/phi-2"
    quantization: "4bit" # or null if you have enough VRAM
    offloading: true

  - name: "llama2-7b"
    variant: "llama"
    size: "7B"
    framework: "huggingface"
    checkpoint: "meta-llama/Llama-2-7b-hf"
    quantization: "4bit"
    offloading: true
    torch_dtype: "bfloat16"

  - name: "llama3-8b"
    variant: "llama3"
    size: "8B"
    framework: "huggingface"
    checkpoint: "meta-llama/Meta-Llama-3-8B"
    quantization: "4bit"
    offloading: true
    torch_dtype: "bfloat16"

  - name: "mistral-7b"
    variant: "mistral"
    size: "7B"
    framework: "huggingface"
    checkpoint: "mistralai/Mistral-7B-v0.1"
    quantization: "4bit"
    offloading: true
    torch_dtype: "bfloat16"

  - name: "llama3-8b 8quant"
    variant: "llama3"
    size: "8B"
    framework: "huggingface"
    checkpoint: "meta-llama/Meta-Llama-3-8B"
    quantization: "8bit"
    offloading: true
    torch_dtype: "float16"

  - name: "mistral-7b 8quant"
    variant: "mistral"
    size: "7B"
    framework: "huggingface"
    checkpoint: "mistralai/Mistral-7B-v0.1"
    quantization: "8bit"
    offloading: true
    torch_dtype: "float16"


# ============================
# Model Parameters Configuration (Global for model loading, not generation)
# ============================
model_parameters:
  max_input_length: 512 # Used by some model loaders, but tokenizer truncation in TaskHandler is key
  max_output_length: 512 # Generic, actual generation length is by max_new_tokens in advanced settings

# ============================
# Evaluation Configuration
# ============================
evaluation:
  log_interval: 5 # Log intermediate metrics every 5 batches

# ============================
# Results and Reporting
# ============================
reporting:
  enabled: true
  format: "json" # Or "pdf", "csv"
  leaderboard_enabled: false
  generate_visuals:
    charts: true
    tables: true
    save_plots: true
  output_dir: "./reports"

# ============================
# Advanced Settings (Global runtime/generation parameters for TaskHandlers)
# ============================
advanced:
  enable_multi_gpu: false
  use_tpu: false
  distributed_training: false
  batch_size: 10      # Main batch_size for DataLoader/inference
  truncation: true    # For tokenizer
  padding: true       # For tokenizer ("longest" or a max_length strategy)
  generate_max_length: 512 # Max length for tokenizer input processing in TaskHandler's _generate_text
  skip_special_tokens: true # For tokenizer.decode
  max_new_tokens: 150  # Max *new* tokens for model.generate(). Increased from 50.
                      # Consider increasing this further for GSM8K (e.g., 256-512) or Summarization,
                      # or allow override in task_cfg.handler_options.
  num_beams: 1
  do_sample: false
  use_cache: true     # Setting to true is generally recommended for faster generation, uses more VRAM.
  clean_up_tokenization_spaces: true
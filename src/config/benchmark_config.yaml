# ============================
# General Configuration
# ============================
general:
  experiment_name: "Gemma_Benchmark_Comprehensive_2025"
  output_dir: "./benchmarks_output" # All benchmark outputs will be saved here
  random_seed: 42

# ============================
# Benchmark Tasks Configuration
# ============================
tasks:
  # --- Task 1: MMLU (Multiple Choice QA) ---
  - name: "MMLU (all Subset - Templated)"
    type: "multiple_choice_qa"
    description: "MMLU (all subjects) via templated letter generation."
    datasets:
      - name: "cais/mmlu"
        source_type: "hf_hub"
        config: "all" # Using all MMLU subjects
        split: "validation"
    handler_options:
      prompt_builder_type: "mmlu" # Uses MMLUPromptBuilder
      prompt_template: |
        The following is a multiple-choice question about {subject}.
        Please choose the letter of the correct answer.
        Question: {question}
        Choices:
        {choices_formatted_str}
        Answer:
      default_subject: "the following topic"
    evaluation_metrics:
      - name: exact_match
        options: { normalize: true, ignore_case: true, ignore_punct: true }

  # --- Task 2: GSM8K (Reasoning Text Analysis) ---
  - name: "GSM8K (Reasoning Text Analysis)"
    type: "math_reasoning_generation"
    description: "Grade school math problems. Evaluates the textual properties of the generated reasoning steps."
    datasets:
      - name: "gsm8k"
        source_type: "hf_hub"
        config: "main"
        split: "test"
    handler_options:
      prompt_builder_type: "default"
      prompt_template: "Solve the following math problem step-by-step. Your final answer should be a number, formatted as ####<number>.\n\nProblem: {input_text}\n\nSolution:"
      postprocessor_key: "default" # Uses DefaultPostProcessor to keep the full reasoning string
    evaluation_metrics:
      - name: rouge # Compares textual similarity of the reasoning steps
        options:
          metrics: ['rouge1', 'rouge2', 'rougeL']
          stats: ['f']
      - name: bert_score # Compares semantic similarity of the reasoning steps
        options: { lang: "en" }
      - name: distinct_ngram # Calculates lexical diversity in the reasoning text
        options: { ngrams: [1, 2, 3] }

  # --- Task 3: Summarization (CNN/DailyMail) ---
  - name: "CNN/DailyMail Summarization"
    type: "summarization"
    description: "Abstractive summarization of news articles."
    datasets:
      - name: "cnn_dailymail"
        source_type: "hf_hub"
        config: "3.0.0"
        split: "test"
    handler_options:
      prompt_builder_type: "default"
      prompt_template: "Summarize the following article in a few sentences:\n\nArticle: {article_text}\n\nSummary:"
    evaluation_metrics:
      - name: rouge
        options:
          metrics: ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
          stats: ['f', 'p', 'r']
      - name: bert_score
        options: { lang: "en" }
      - name: distinct_ngram
        options: { ngrams: [1, 2, 3] }

  # --- Task 4: Translation (OPUS-100 English to French) ---
  - name: "OPUS-100 English-to-French"
    type: "translation"
    description: "Translate sentences from English to French using OPUS-100."
    datasets:
      - name: "opus100"
        source_type: "hf_hub"
        config: "en-fr" # This is passed as 'dataset_config_name' to the handler
        split: "validation"
    handler_options:
      prompt_builder_type: "translation" # Uses TranslationPromptBuilder
      prompt_template: "Translate the following {source_lang_name} text into {target_lang_name}: {input_text}"
    evaluation_metrics:
      - name: bleu
        options: {} # Uses default BLEU settings in your metric
      - name: meteor
        options: {} # Uses default METEOR settings

# ============================
# Models Configuration
# ============================
models:
  - name: "gemma-7b-it" # Using instruction-tuned version
    variant: "gemma"
    size: "7B"
    framework: "huggingface"
    checkpoint: "google/gemma-7b-it"
    quantization: "4bit" # Or "8bit" or null (None in Python)
    offloading: true
  - name: "gemma-2b-it"
    variant: "gemma"
    size: "2B"
    framework: "huggingface"
    checkpoint: "google/gemma-2b-it"
    quantization: "8bit"
    offloading: true
  - name: "Llama-2-7b" # Open Llama 3B model as a ~2.7B Llama option
    variant: "llama"
    size: "7B"
    framework: "huggingface"
    checkpoint: "meta-llama/Llama-2-7b"
    quantization: "4bit"
    offloading: true
  - name: "gpt2" # Added GPT-2 base model
    variant: "gpt2"
    size: "124M" # Standard GPT-2 small
    framework: "huggingface"
    checkpoint: "gpt2"
    quantization: null # No quantization for gpt2, it's small
    offloading: false
  # - name: "phi-2" # Alternative 2.7B parameter model (not Llama architecture)
  #   variant: "phi"
  #   size: "2.7B"
  #   framework: "huggingface"
  #   checkpoint: "microsoft/phi-2"
  #   quantization: "4bit" # or null if you have enough VRAM
  #   offloading: true

# ============================
# Model Parameters Configuration (Global for model loading, not generation)
# ============================
model_parameters:
  max_input_length: 512 # Used by some model loaders, but tokenizer truncation in TaskHandler is key
  max_output_length: 512 # Generic, actual generation length is by max_new_tokens in advanced settings

# ============================
# Evaluation Configuration
# ============================
evaluation:
  log_interval: 5 # Log intermediate metrics every 5 batches

# ============================
# Results and Reporting
# ============================
reporting:
  enabled: true
  format: "json" # Or "pdf", "csv"
  leaderboard_enabled: false
  generate_visuals:
    charts: true
    tables: true
    save_plots: true
  output_dir: "./reports"

# ============================
# Advanced Settings (Global runtime/generation parameters for TaskHandlers)
# ============================
advanced:
  enable_multi_gpu: false
  use_tpu: false
  distributed_training: false
  batch_size: 8       # Main batch_size for DataLoader/inference
  truncation: true    # For tokenizer
  padding: true       # For tokenizer ("longest" or a max_length strategy)
  generate_max_length: 512 # Max length for tokenizer input processing in TaskHandler's _generate_text
  skip_special_tokens: true # For tokenizer.decode
  max_new_tokens: 150  # Max *new* tokens for model.generate(). Increased from 50.
                      # Consider increasing this further for GSM8K (e.g., 256-512) or Summarization,
                      # or allow override in task_cfg.handler_options.
  num_beams: 1
  do_sample: false
  use_cache: true     # Setting to true is generally recommended for faster generation, uses more VRAM.
  clean_up_tokenization_spaces: true